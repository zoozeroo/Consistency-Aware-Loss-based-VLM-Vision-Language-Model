{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "QBA6vCcZbqyj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/six-1.17.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/contourpy-1.3.2-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/kiwisolver-1.4.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/cycler-0.12.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pyparsing-3.2.3-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/fonttools-4.58.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/matplotlib-3.10.3-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/python_dateutil-2.9.0.post0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pycocotools-2.0.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: open_clip_torch in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.20.1+cu121)\n",
      "Requirement already satisfied: regex in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.32.0)\n",
      "Requirement already satisfied: safetensors in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open_clip_torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (1.1.2)\n",
      "Requirement already satisfied: numpy in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torchvision->open_clip_torch) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torchvision->open_clip_torch) (11.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.4.26)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "BEAYQGGNd0Jd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# OpenCLIP 모델 및 전처리 로딩 (사전학습된 모델 사용)\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32',\n",
    "    pretrained='laion2b_s34b_b79k'  # 또는 'openai' 등\n",
    ")\n",
    "model.to(device)\n",
    "model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "wTbGumbid3ls"
   },
   "outputs": [],
   "source": [
    "# COCO 이미지 및 캡션 경로 설정\n",
    "image_dir = \"train2017\"\n",
    "caption_json_path = \"annotations/captions_train2017.json\"\n",
    "\n",
    "# 캡션 로드 및 매핑\n",
    "with open(caption_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "imgid_to_captions = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    imgid_to_captions[ann['image_id']].append(ann['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "gJR5Hib4d74J"
   },
   "outputs": [],
   "source": [
    "# 이미지 3장 샘플링\n",
    "image_list = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "sample_images = random.sample(image_list, 3)\n",
    "\n",
    "# 샘플 이미지 3장, 각 이미지에 대해 caption 5개 추출\n",
    "image_paths = []\n",
    "captions_list = []\n",
    "\n",
    "for fname in sample_images:\n",
    "    image_id = int(os.path.splitext(fname)[0])\n",
    "    cap_list = imgid_to_captions.get(image_id, [])\n",
    "    if cap_list:\n",
    "        image_paths.append(os.path.join(image_dir, fname))\n",
    "        # 최대 5개 캡션 사용\n",
    "        captions_list.append(cap_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "DcP8fz5jd-ex"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 이미지:\n",
      "- 000000376131.jpg → \"A computer is used as a tool by the medical professional.\"\n",
      "- 000000144929.jpg → \"A young girl jumping into the air while holding an umbrella.\"\n",
      "- 000000088272.jpg → \"A plate containing a piece of pizza, sauce, knife and fork.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"샘플 이미지:\")\n",
    "# 각 이미지당 대표 캡션 1개만 선택\n",
    "captions = [caps[0] for caps in captions_list]\n",
    "for path, cap in zip(image_paths, captions):\n",
    "    print(f\"- {os.path.basename(path)} → \\\"{cap}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "1-kKDrtreBQw"
   },
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 임베딩\n",
    "images = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]).to(device)\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 캡션 임베딩\n",
    "tokenized = tokenizer(captions).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(tokenized)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "cS07a4iTeEVe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " OpenCLIP Cosine Similarity Matrix:\n",
      "\n",
      " 000000376131.jpg\n",
      "  \"A computer is used as a tool by the medical professional.\" → similarity: 0.2595\n",
      "  \"A young girl jumping into the air while holding an umbrella.\" → similarity: -0.0219\n",
      "  \"A plate containing a piece of pizza, sauce, knife and fork.\" → similarity: 0.0491\n",
      "\n",
      " 000000144929.jpg\n",
      "  \"A computer is used as a tool by the medical professional.\" → similarity: 0.0451\n",
      "  \"A young girl jumping into the air while holding an umbrella.\" → similarity: 0.2786\n",
      "  \"A plate containing a piece of pizza, sauce, knife and fork.\" → similarity: 0.0012\n",
      "\n",
      " 000000088272.jpg\n",
      "  \"A computer is used as a tool by the medical professional.\" → similarity: 0.0679\n",
      "  \"A young girl jumping into the air while holding an umbrella.\" → similarity: 0.0563\n",
      "  \"A plate containing a piece of pizza, sauce, knife and fork.\" → similarity: 0.3487\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity 계산\n",
    "image_embeds_np = image_embeds.cpu().numpy()\n",
    "text_embeds_np = text_embeds.cpu().numpy()\n",
    "sims = cosine_similarity(image_embeds_np, text_embeds_np)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n OpenCLIP Cosine Similarity Matrix:\")\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    print(f\"\\n {os.path.basename(img_path)}\")\n",
    "    for j, cap in enumerate(captions):\n",
    "        print(f\"  \\\"{cap}\\\" → similarity: {sims[i, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정확히 매칭된 쌍 유사도:\n",
      "000000376131.jpg ⟶ \"A computer is used as a tool by the medical professional.\" → similarity: 0.2595\n",
      "000000144929.jpg ⟶ \"A young girl jumping into the air while holding an umbrella.\" → similarity: 0.2786\n",
      "000000088272.jpg ⟶ \"A plate containing a piece of pizza, sauce, knife and fork.\" → similarity: 0.3487\n"
     ]
    }
   ],
   "source": [
    "# 이미지-텍스트 쌍 (정확히 매칭된 1:1 쌍만)\n",
    "captions = [caps[0] for caps in captions_list]  # 각 이미지의 첫 번째 캡션을 정답으로 사용\n",
    "\n",
    "# 이미지 임베딩\n",
    "images = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]).to(device).float()\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 텍스트 임베딩\n",
    "tokenized = tokenizer(captions).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(tokenized)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Cosine similarity 계산\n",
    "sims = cosine_similarity(image_embeds.cpu().numpy(), text_embeds.cpu().numpy())\n",
    "\n",
    "# 정답 쌍 유사도만 출력\n",
    "print(\"\\n정확히 매칭된 쌍 유사도:\")\n",
    "for i in range(len(image_paths)):\n",
    "    print(f\"{os.path.basename(image_paths[i])} ⟶ \\\"{captions[i]}\\\" → similarity: {sims[i, i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenCLIP Cosine Similarity (Best-of-5 Captions):\n",
      "\n",
      " 000000376131.jpg\n",
      " Best caption: \"A doctor's office counter cover in papers and a laptop\"\n",
      " similarity: 0.3775\n",
      "\n",
      " 000000144929.jpg\n",
      " Best caption: \"A young girl wearing shorts leaps into the air while holding an umbrella.\"\n",
      " similarity: 0.2960\n",
      "\n",
      " 000000088272.jpg\n",
      " Best caption: \"A piece of pizza and mustard are on a plate.\"\n",
      " similarity: 0.3487\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# 1. 이미지 임베딩\n",
    "images = torch.stack([\n",
    "    preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "]).to(device).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 2. 각 이미지에 대해 5개 캡션 중 best one 선택\n",
    "best_text_feats = []\n",
    "best_captions = []\n",
    "\n",
    "for i, caps in enumerate(captions_list):  # captions_list[i] : i번째 이미지의 5개 캡션\n",
    "    tokenized = tokenizer(caps).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeds_5 = model.encode_text(tokenized)\n",
    "        text_embeds_5 = text_embeds_5 / text_embeds_5.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 이미지 i번째와 5개 텍스트 임베딩 간 cosine 유사도 계산\n",
    "        sims = torch.matmul(image_embeds[i].unsqueeze(0), text_embeds_5.T).squeeze(0)\n",
    "        best_idx = sims.argmax().item()\n",
    "        best_text_feats.append(text_embeds_5[best_idx].unsqueeze(0))\n",
    "        best_captions.append(caps[best_idx])  # 해당 캡션 저장\n",
    "\n",
    "text_embeds = torch.cat(best_text_feats, dim=0)\n",
    "\n",
    "# 3. Cosine similarity (diagonal만)\n",
    "sims = cosine_similarity(image_embeds.cpu().numpy(), text_embeds.cpu().numpy())\n",
    "\n",
    "# 4. 출력\n",
    "print(\"\\nOpenCLIP Cosine Similarity (Best-of-5 Captions):\")\n",
    "for i in range(len(image_paths)):\n",
    "    print(f\"\\n {os.path.basename(image_paths[i])}\")\n",
    "    print(f\" Best caption: \\\"{best_captions[i]}\\\"\")\n",
    "    print(f\" similarity: {sims[i, i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (wasp)",
   "language": "python",
   "name": "wasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
