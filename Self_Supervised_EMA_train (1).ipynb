{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val ['000000433103.jpg', '000000129113.jpg', '000000196843.jpg', '000000252507.jpg', '000000258541.jpg']\n",
      "train ['000000254879.jpg', '000000316649.jpg', '000000430989.jpg', '000000286349.jpg', '000000458365.jpg']\n",
      "val파일 개수: 5000\n",
      "train파일 개수: 109932\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "val = \"val2017\"\n",
    "train_path = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "val_path = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "# folder_path = \"/raid/kyscap251/team2/val2017/val2017\"\n",
    "test = \"train2017\"\n",
    "\n",
    "val_items = os.listdir(val_path)\n",
    "train_items = os.listdir(train_path)\n",
    "\n",
    "print(\"val\", val_items[:5])\n",
    "print(\"train\", train_items[:5])\n",
    "\n",
    "# 파일만 필터링\n",
    "files = [f for f in os.listdir(val_path)\n",
    "         if os.path.isfile(os.path.join(val_path, f))]\n",
    "\n",
    "print(f\"val파일 개수: {len(files)}\")\n",
    "\n",
    "filess = [ff for ff in os.listdir(train_path)\n",
    "         if os.path.isfile(os.path.join(train_path, ff))]\n",
    "print(f\"train파일 개수: {len(filess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정 및 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이미지 인코더: ViT\n",
    "# class VisionEncoder(nn.Module):\n",
    "#     def __init__(self, output_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.vit = models.vit_b_16(pretrained=True)\n",
    "#         self.vit.heads = nn.Identity()  # classification head 제거\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         patch_feats = self.vit(images)  # [B, D]\n",
    "#         return patch_feats\n",
    "    \n",
    "\n",
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 인코더: BERT 기반 Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델\n",
    "# class VisionLanguageModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.vision_encoder = VisionEncoder()\n",
    "#         self.text_encoder = TextEncoder()\n",
    "#         self.cross_attn = CrossAttentionBlock()\n",
    "#         self.proj_image = nn.Linear(768, 512)\n",
    "#         self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "#     def forward(self, images, captions):\n",
    "#         img_feat = self.vision_encoder(images)\n",
    "#         text_emb, tokens = self.text_encoder(captions)\n",
    "#         patch_feat = img_feat.unsqueeze(1)  # dummy patch feature (B, 1, D)\n",
    "#         cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)\n",
    "#         img_proj = self.proj_image(img_feat)\n",
    "#         text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "#     def encode_for_inference(self, images, captions):\n",
    "#         with torch.no_grad():\n",
    "#             img_feat = self.vision_encoder(images)\n",
    "#             text_emb, _ = self.text_encoder(captions)\n",
    "#             img_proj = self.proj_image(img_feat)\n",
    "#             text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj\n",
    "    \n",
    "# -------------------------------\n",
    "# Vision-Language Model\n",
    "# -------------------------------\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #랜덤 3개 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #평균 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS token만 추출 (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss 계산 함수\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 JSON 로딩 및 binary mask 생성\n",
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.data = []\n",
    "        for entry in all_data:\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"유효 이미지 수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "#             print(f\"[WARN] 이미지 불러오기 실패: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # 다음 인덱스로 재시도\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        masks = torch.zeros((self.max_tokens, H, W))\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            masks[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "        return image, caption, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    captions = [item[1] for item in batch]\n",
    "    masks = torch.stack([item[2] for item in batch])\n",
    "    return images, captions, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효 이미지 수: 109932\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=\"coco_token_bbox_matched.json\",\n",
    "    image_root=\"/raid/kyscap251/team2/train2017/train2017\",\n",
    "#     image_root = \"/shared/home/kyscap251/Team2/val2017\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=coco_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #람다 점진적 높이기\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn, curr_attn: [B, T, P] - softmax된 attention map\n",
    "    \"\"\"\n",
    "    prev = torch.clamp(prev_attn, min=eps)\n",
    "    curr = torch.clamp(curr_attn, min=eps)\n",
    "    kl = (prev * (prev.log() - curr.log())).sum(dim=-1).mean()  # 평균 over T, B\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.ema = None\n",
    "\n",
    "    def update(self, value):\n",
    "        with torch.no_grad():\n",
    "            if self.ema is None:\n",
    "                self.ema = value.detach()\n",
    "            else:\n",
    "                self.ema = self.alpha * value.detach() + (1 - self.alpha) * self.ema\n",
    "        return self.ema\n",
    "\n",
    "    def reset(self):\n",
    "        self.ema = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device,\n",
    "                lambda_cons=0.1,\n",
    "                lambda_self=0.1,\n",
    "                warmup_epochs=3,\n",
    "                num_epochs=5,\n",
    "                start_epoch=0,\n",
    "                prev_attn_dict=None,\n",
    "                ema_tracker=None):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # EMA 초기화\n",
    "    if ema_tracker is None:\n",
    "        ema_tracker = EMA(alpha=0.2)\n",
    "    use_ema = False\n",
    "\n",
    "    # Lambda scheduler는 warm-up 동안만 사용\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\", leave=False)\n",
    "\n",
    "        if epoch == warmup_epochs:\n",
    "            ema_tracker.reset()\n",
    "            use_ema = True\n",
    "            print(f\"[Epoch {epoch}] EMA 기준 self-consistency 시작\")\n",
    "\n",
    "        for step, (images, captions, masks) in enumerate(progress):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :]\n",
    "\n",
    "            # ▶ Contrastive Loss (공통)\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            # ▶ Warm-up Phase\n",
    "            if epoch < warmup_epochs:\n",
    "                lambda_val, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "                loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "                loss = loss_contrastive + lambda_val * loss_consistency\n",
    "\n",
    "            # ▶ Self-consistency Phase\n",
    "            else:\n",
    "                ema_attn = ema_tracker.update(attn_weights_matched)\n",
    "                loss_self_cons = kl_divergence_attention(ema_attn, attn_weights_matched.detach())\n",
    "                loss = loss_contrastive + lambda_self * loss_self_cons\n",
    "\n",
    "            # ▶ Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ▶ Accuracy\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "            if step % 50 == 0 or step == len(dataloader) - 1:\n",
    "                progress.set_postfix({\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"acc\": f\"{acc:.3f}\",\n",
    "                    \"λ\": f\"{lambda_cons:.3f}\" if epoch < warmup_epochs else f\"{lambda_self:.3f}\"\n",
    "                })\n",
    "\n",
    "        # ▶ Save checkpoint after warm-up phase\n",
    "        if epoch + 1 == warmup_epochs:\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'attn_dict': attn_weights_matched.detach().clone()\n",
    "            }, f'checkpoint_epoch{epoch+1}.pth')\n",
    "            print(f\"[Checkpoint] Saved at epoch {epoch+1}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "\n",
    "    return ema_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2:   0%| | 1/27483 [00:01<10:06:19,  1.32s/it, loss=1.3262, acc=0.500, λ"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 6.4150, Avg Acc: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved at epoch 2\n",
      "Epoch 2 - Avg Loss: 6.8522, Avg Acc: 0.6867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EMA at 0x7f41086cd9a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델과 옵티마이저 초기화\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Warm-up 전용 학습 (예: 2 epoch 동안)\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.05,         # warm-up용 consistency loss 계수\n",
    "    lambda_self=0.0,          # 후기용 self-consistency는 사용 안 함\n",
    "    warmup_epochs=2,\n",
    "    num_epochs=2              # warm-up만 돌리므로 일치시킴\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343495/4206991403.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint_epoch2.pth')\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 1.4403, Avg Acc: 0.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 1.3967, Avg Acc: 0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 1.3962, Avg Acc: 0.2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7:  80%|▊| 21858/27483 [2:55:26<43:56,  2.13it/s, loss=1.3868, acc=0.250IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 7/7:  25%|▎| 6958/27483 [56:42<2:37:58,  2.17it/s, loss=1.3884, acc=0.250,IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 1.3881, Avg Acc: 0.2494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EMA at 0x7f4111cd2120>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint_epoch2.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "prev_attn_map = checkpoint['attn_dict']\n",
    "\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.0,               # warm-up 끝났으므로 사용 안 함\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=0,\n",
    "    num_epochs=5,                  # 후속 학습\n",
    "    start_epoch=2,                 # 이어서\n",
    "    prev_attn_dict=prev_attn_map   # 필요시 활용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Epoch 학습 루프 (Matched Token만 사용하도록 ㅏ수정)\n",
    "def train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "#     ////////////\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "#     ////////////////////\n",
    "#     lambda_log = [] #λ 변화 기록용 리스트\n",
    "#     diff_log = [] #diff값 기록용 리스트\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            # matched token 수만큼 attention weight 슬라이싱\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :] #bbox에 해당하는 token만 consistency loss 계산에 사용\n",
    "\n",
    "            \n",
    "#             //////////////\n",
    "            # 현재 λ 값을 EMA 기반으로 계산 및 기록\n",
    "            lambda_cons, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "#             lambda_log.append(lambda_cons)\n",
    "#             diff_log.append(diff)\n",
    "# /////////////////////////////////////\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # cosine similarity + accuracy 계산\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                sims = torch.diag(sim_matrix)\n",
    "                sim_mean = sims.mean().item()\n",
    "                sim_std = sims.std().item()\n",
    "\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"cos_sim\": f\"{sim_mean:.3f}±{sim_std:.3f}\", \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "#         # scheduler 업데이트\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "#             print(f\"[Epoch {epoch+1}] LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4248c3f7bd4517bfa9c8e008b28be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d1bfc43cb46968e6b8427c9d42bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Avg Loss: 0.9415, Avg Accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Avg Loss: 0.7531, Avg Accuracy: 0.9510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Avg Loss: 0.7454, Avg Accuracy: 0.9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# # 모델 학습 실행\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 3개 추론\n",
    "def run_batch_inference(model, val_image_dir, caption_json_path, transform, device, sample_size=3):\n",
    "    with open(caption_json_path, 'r') as f:\n",
    "        coco_captions = json.load(f)\n",
    "\n",
    "    # image_id → caption 매핑\n",
    "    imgid2caption = {}\n",
    "    for ann in coco_captions['annotations']:\n",
    "        imgid = ann['image_id']\n",
    "        if imgid not in imgid2caption:\n",
    "            imgid2caption[imgid] = []\n",
    "        imgid2caption[imgid].append(ann['caption'])\n",
    "\n",
    "    # 랜덤 샘플링 (image_id 3개)\n",
    "    img_ids = random.sample(list(imgid2caption.keys()), sample_size)\n",
    "    captions = [imgid2caption[i][0] for i in img_ids]\n",
    "    image_paths = [os.path.join(val_image_dir, f\"{i:012d}.jpg\") for i in img_ids]\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "    ]).to(device)\n",
    "\n",
    "    # 인코딩\n",
    "    model.eval()\n",
    "    image_embeds, text_embeds = model.encode_for_inference(images_tensor, captions)\n",
    "\n",
    "    # 코사인 유사도\n",
    "    sim_matrix = F.cosine_similarity(image_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=-1)\n",
    "\n",
    "    # 출력\n",
    "    print(\"image_embeds shape:\", image_embeds.shape)\n",
    "    print(\"text_embeds shape :\", text_embeds.shape)\n",
    "    print(\"\\n\\U0001F4CA Cosine Similarity Matrix:\\n\")\n",
    "    for i, img_id in enumerate(img_ids):\n",
    "        print(f\"\\U0001F5BC️ {img_id:012d}.jpg\")\n",
    "        for j, cap in enumerate(captions):\n",
    "            print(f\"  \\\"{cap}\\\" → similarity: {sim_matrix[i, j]:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_inference(\n",
    "    model,\n",
    "    val_image_dir=\"val2017\",\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    sample_size=3 #랜덤으로 이미지 세장\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.imgid2caption:\n",
    "                self.imgid2caption[img_id] = ann['caption']\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        caption = self.imgid2caption[img_id]\n",
    "\n",
    "        encoding = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "val_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답,오답쌍 평균 유사도 계산 추론\n",
    "def evaluate_mean_similarity(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct_sim = 0.0\n",
    "    total_incorrect_sim = 0.0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)  # (B, 512)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(\n",
    "                image_feat.unsqueeze(1),  # (B, 1, D)\n",
    "                text_feat.unsqueeze(0),  # (1, B, D)\n",
    "                dim=-1\n",
    "            )  # (B, B)\n",
    "\n",
    "            B = sim_matrix.size(0)\n",
    "            correct_sims = sim_matrix.diag()\n",
    "            total_correct_sim += correct_sims.sum().item()\n",
    "            correct_count += B\n",
    "\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "            incorrect_sims = sim_matrix[mask]\n",
    "            total_incorrect_sim += incorrect_sims.sum().item()\n",
    "            incorrect_count += incorrect_sims.numel()\n",
    "\n",
    "    mean_correct = total_correct_sim / correct_count\n",
    "    mean_incorrect = total_incorrect_sim / incorrect_count\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\" - Mean Correct Sim    : {mean_correct:.4f}\")\n",
    "    print(f\" - Mean Incorrect Sim  : {mean_incorrect:.4f}\")\n",
    "    return mean_correct, mean_incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 157/157 [01:19<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      " - Mean Correct Sim    : -0.0597\n",
      " - Mean Incorrect Sim  : -0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_correct, mean_incorrect = evaluate_mean_similarity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후기단계 시작\n"
     ]
    }
   ],
   "source": [
    "print(\"후기단계 시작\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.9405, Avg Accuracy: 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.7593, Avg Accuracy: 0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.7222, Avg Accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.05,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=2,\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.1864, Avg Accuracy: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.1822, Avg Accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.1588, Avg Accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.1380, Avg Accuracy: 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.1259, Avg Accuracy: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.1208, Avg Accuracy: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.1105, Avg Accuracy: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint_epoch3.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "prev_attn_map = checkpoint['attn_map']\n",
    "\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.0,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=0,\n",
    "    num_epochs=7,\n",
    "    initial_attn_map=prev_attn_map,\n",
    "    start_epoch=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wasp)",
   "language": "python",
   "name": "wasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
