{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QBA6vCcZbqyj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/six-1.17.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/contourpy-1.3.2-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/kiwisolver-1.4.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/cycler-0.12.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pyparsing-3.2.3-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/fonttools-4.58.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/matplotlib-3.10.3-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/python_dateutil-2.9.0.post0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/pycocotools-2.0.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: open_clip_torch in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.20.1+cu121)\n",
      "Requirement already satisfied: regex in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.32.0)\n",
      "Requirement already satisfied: safetensors in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open_clip_torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (1.1.2)\n",
      "Requirement already satisfied: numpy in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torchvision->open_clip_torch) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from torchvision->open_clip_torch) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.4.26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/home/kyscap251/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.4.26)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BEAYQGGNd0Jd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# OpenCLIP 모델 및 전처리 로딩 (사전학습된 모델 사용)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B-32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaion2b_s34b_b79k\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 또는 'openai' 등\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B-32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 기본 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# OpenCLIP 모델 및 전처리 로딩 (사전학습된 모델 사용)\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32',\n",
    "    pretrained='laion2b_s34b_b79k'  # 또는 'openai' 등\n",
    ")\n",
    "tokenizer = get_tokenizer('ViT-B-32')\n",
    "model.to(device)\n",
    "model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTbGumbid3ls"
   },
   "outputs": [],
   "source": [
    "# COCO 이미지 및 캡션 경로 설정\n",
    "image_dir = \"val2017\"\n",
    "caption_json_path = \"annotations/captions_val2017.json\"\n",
    "\n",
    "# 캡션 로드 및 매핑\n",
    "with open(caption_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "imgid_to_captions = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    imgid_to_captions[ann['image_id']].append(ann['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJR5Hib4d74J"
   },
   "outputs": [],
   "source": [
    "# 이미지 3장 샘플링\n",
    "image_list = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "sample_images = random.sample(image_list, 3)\n",
    "\n",
    "# 샘플 이미지 3장, 각 이미지에 대해 caption 5개 추출\n",
    "image_paths = []\n",
    "captions_list = []\n",
    "\n",
    "for fname in sample_images:\n",
    "    image_id = int(os.path.splitext(fname)[0])\n",
    "    cap_list = imgid_to_captions.get(image_id, [])\n",
    "    if cap_list:\n",
    "        image_paths.append(os.path.join(image_dir, fname))\n",
    "        # 최대 5개 캡션 사용\n",
    "        captions_list.append(cap_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcP8fz5jd-ex"
   },
   "outputs": [],
   "source": [
    "print(\"샘플 이미지:\")\n",
    "# 각 이미지당 대표 캡션 1개만 선택\n",
    "captions = [caps[0] for caps in captions_list]\n",
    "for path, cap in zip(image_paths, captions):\n",
    "    print(f\"- {os.path.basename(path)} → \\\"{cap}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-kKDrtreBQw"
   },
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 임베딩\n",
    "images = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]).to(device)\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 캡션 임베딩\n",
    "tokenized = tokenizer(captions).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(tokenized)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cS07a4iTeEVe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_embeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cosine Similarity 계산\u001b[39;00m\n\u001b[1;32m      2\u001b[0m image_embeds_np \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 3\u001b[0m text_embeds_np \u001b[38;5;241m=\u001b[39m \u001b[43mtext_embeds\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m sims \u001b[38;5;241m=\u001b[39m cosine_similarity(image_embeds_np, text_embeds_np)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_embeds' is not defined"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity 계산\n",
    "image_embeds_np = image_embeds.cpu().numpy()\n",
    "text_embeds_np = text_embeds.cpu().numpy()\n",
    "sims = cosine_similarity(image_embeds_np, text_embeds_np)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n OpenCLIP Cosine Similarity Matrix:\")\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    print(f\"\\n {os.path.basename(img_path)}\")\n",
    "    for j, cap in enumerate(captions):\n",
    "        print(f\"  \\\"{cap}\\\" → similarity: {sims[i, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([preprocess(Image\u001b[38;5;241m.\u001b[39mopen(p)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m image_paths])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m image_embeds \u001b[38;5;241m/\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 텍스트 임베딩\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/open_clip/model.py:279\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 279\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/open_clip/transformer.py:826\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 826\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m    828\u001b[0m     pooled, tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/open_clip/transformer.py:702\u001b[0m, in \u001b[0;36mVisionTransformer._embeds\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embeds\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 702\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, dim, grid, grid]\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wasp/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# 이미지-텍스트 쌍 (정확히 매칭된 1:1 쌍만)\n",
    "captions = [caps[0] for caps in captions_list]  # 각 이미지의 첫 번째 캡션을 정답으로 사용\n",
    "\n",
    "# 이미지 임베딩\n",
    "images = torch.stack([preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]).to(device).float()\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 텍스트 임베딩\n",
    "tokenized = tokenizer(captions).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(tokenized)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Cosine similarity 계산\n",
    "sims = cosine_similarity(image_embeds.cpu().numpy(), text_embeds.cpu().numpy())\n",
    "\n",
    "# 정답 쌍 유사도만 출력\n",
    "print(\"\\n정확히 매칭된 쌍 유사도:\")\n",
    "for i in range(len(image_paths)):\n",
    "    print(f\"{os.path.basename(image_paths[i])} ⟶ \\\"{captions[i]}\\\" → similarity: {sims[i, i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# 1. 이미지 임베딩\n",
    "images = torch.stack([\n",
    "    preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "]).to(device).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.encode_image(images)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "# 2. 각 이미지에 대해 5개 캡션 중 best one 선택\n",
    "best_text_feats = []\n",
    "best_captions = []\n",
    "\n",
    "for i, caps in enumerate(captions_list):  # captions_list[i] : i번째 이미지의 5개 캡션\n",
    "    tokenized = tokenizer(caps).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeds_5 = model.encode_text(tokenized)\n",
    "        text_embeds_5 = text_embeds_5 / text_embeds_5.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 이미지 i번째와 5개 텍스트 임베딩 간 cosine 유사도 계산\n",
    "        sims = torch.matmul(image_embeds[i].unsqueeze(0), text_embeds_5.T).squeeze(0)\n",
    "        best_idx = sims.argmax().item()\n",
    "        best_text_feats.append(text_embeds_5[best_idx].unsqueeze(0))\n",
    "        best_captions.append(caps[best_idx])  # 해당 캡션 저장\n",
    "\n",
    "text_embeds = torch.cat(best_text_feats, dim=0)\n",
    "\n",
    "# 3. Cosine similarity (diagonal만)\n",
    "sims = cosine_similarity(image_embeds.cpu().numpy(), text_embeds.cpu().numpy())\n",
    "\n",
    "# 4. 출력\n",
    "print(\"\\nOpenCLIP Cosine Similarity (Best-of-5 Captions):\")\n",
    "for i in range(len(image_paths)):\n",
    "    print(f\"\\n {os.path.basename(image_paths[i])}\")\n",
    "    print(f\" Best caption: \\\"{best_captions[i]}\\\"\")\n",
    "    print(f\" similarity: {sims[i, i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (wasp)",
   "language": "python",
   "name": "wasp310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
