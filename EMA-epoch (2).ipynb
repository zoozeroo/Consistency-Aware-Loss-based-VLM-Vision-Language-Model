{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val ['000000433103.jpg', '000000129113.jpg', '000000196843.jpg', '000000252507.jpg', '000000258541.jpg']\n",
      "train ['000000427548.jpg', '000000367442.jpg', '000000574946.jpg', '000000215255.jpg', '000000016119.jpg']\n",
      "val파일 개수: 5000\n",
      "train파일 개수: 109933\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "val = \"val2017\"\n",
    "train_path = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "val_path = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "# folder_path = \"/raid/kyscap251/team2/val2017/val2017\"\n",
    "test = \"train2017\"\n",
    "\n",
    "val_items = os.listdir(val_path)\n",
    "train_items = os.listdir(train_path)\n",
    "\n",
    "print(\"val\", val_items[:5])\n",
    "print(\"train\", train_items[:5])\n",
    "\n",
    "# 파일만 필터링\n",
    "files = [f for f in os.listdir(val_path)\n",
    "         if os.path.isfile(os.path.join(val_path, f))]\n",
    "\n",
    "print(f\"val파일 개수: {len(files)}\")\n",
    "\n",
    "filess = [ff for ff in os.listdir(train_path)\n",
    "         if os.path.isfile(os.path.join(train_path, ff))]\n",
    "print(f\"train파일 개수: {len(filess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정 및 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이미지 인코더: ViT\n",
    "# class VisionEncoder(nn.Module):\n",
    "#     def __init__(self, output_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.vit = models.vit_b_16(pretrained=True)\n",
    "#         self.vit.heads = nn.Identity()  # classification head 제거\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         patch_feats = self.vit(images)  # [B, D]\n",
    "#         return patch_feats\n",
    "    \n",
    "\n",
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 인코더: BERT 기반 Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델\n",
    "# class VisionLanguageModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.vision_encoder = VisionEncoder()\n",
    "#         self.text_encoder = TextEncoder()\n",
    "#         self.cross_attn = CrossAttentionBlock()\n",
    "#         self.proj_image = nn.Linear(768, 512)\n",
    "#         self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "#     def forward(self, images, captions):\n",
    "#         img_feat = self.vision_encoder(images)\n",
    "#         text_emb, tokens = self.text_encoder(captions)\n",
    "#         patch_feat = img_feat.unsqueeze(1)  # dummy patch feature (B, 1, D)\n",
    "#         cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)\n",
    "#         img_proj = self.proj_image(img_feat)\n",
    "#         text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "#     def encode_for_inference(self, images, captions):\n",
    "#         with torch.no_grad():\n",
    "#             img_feat = self.vision_encoder(images)\n",
    "#             text_emb, _ = self.text_encoder(captions)\n",
    "#             img_proj = self.proj_image(img_feat)\n",
    "#             text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj\n",
    "    \n",
    "# -------------------------------\n",
    "# Vision-Language Model\n",
    "# -------------------------------\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #랜덤 3개 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #평균 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS token만 추출 (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss 계산 함수\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 JSON 로딩 및 binary mask 생성\n",
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.data = []\n",
    "        for entry in all_data:\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"유효 이미지 수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            print(f\"[WARN] 이미지 불러오기 실패: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # 다음 인덱스로 재시도\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        masks = torch.zeros((self.max_tokens, H, W))\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            masks[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "        return image, caption, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    captions = [item[1] for item in batch]\n",
    "    masks = torch.stack([item[2] for item in batch])\n",
    "    return images, captions, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효 이미지 수: 5000\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=\"val_coco_token_bbox_matched.json\",\n",
    "#    image_root=\"/raid/kyscap251/team2/val2017/val2017\",\n",
    "    image_root = \"/shared/home/kyscap251/Team2/val2017\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=coco_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #람다 점진적 높이기\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Epoch 학습 루프 (Matched Token만 사용하도록 수정)\n",
    "def train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "#     ////////////\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "#     ////////////////////\n",
    "#     lambda_log = [] #λ 변화 기록용 리스트\n",
    "#     diff_log = [] #diff값 기록용 리스트\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            # matched token 수만큼 attention weight 슬라이싱\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :] #bbox에 해당하는 token만 consistency loss 계산에 사용\n",
    "\n",
    "            \n",
    "#             //////////////\n",
    "            # 현재 λ 값을 EMA 기반으로 계산 및 기록\n",
    "            lambda_cons, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "#             lambda_log.append(lambda_cons)\n",
    "#             diff_log.append(diff)\n",
    "# /////////////////////////////////////\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # cosine similarity + accuracy 계산\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                sims = torch.diag(sim_matrix)\n",
    "                sim_mean = sims.mean().item()\n",
    "                sim_std = sims.std().item()\n",
    "\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"cos_sim\": f\"{sim_mean:.3f}±{sim_std:.3f}\", \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "#         # scheduler 업데이트\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "#             print(f\"[Epoch {epoch+1}] LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 학습 실행\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 3개 추론\n",
    "def run_batch_inference(model, val_image_dir, caption_json_path, transform, device, sample_size=3):\n",
    "    with open(caption_json_path, 'r') as f:\n",
    "        coco_captions = json.load(f)\n",
    "\n",
    "    # image_id → caption 매핑\n",
    "    imgid2caption = {}\n",
    "    for ann in coco_captions['annotations']:\n",
    "        imgid = ann['image_id']\n",
    "        if imgid not in imgid2caption:\n",
    "            imgid2caption[imgid] = []\n",
    "        imgid2caption[imgid].append(ann['caption'])\n",
    "\n",
    "    # 랜덤 샘플링 (image_id 3개)\n",
    "    img_ids = random.sample(list(imgid2caption.keys()), sample_size)\n",
    "    captions = [imgid2caption[i][0] for i in img_ids]\n",
    "    image_paths = [os.path.join(val_image_dir, f\"{i:012d}.jpg\") for i in img_ids]\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "    ]).to(device)\n",
    "\n",
    "    # 인코딩\n",
    "    model.eval()\n",
    "    image_embeds, text_embeds = model.encode_for_inference(images_tensor, captions)\n",
    "\n",
    "    # 코사인 유사도\n",
    "    sim_matrix = F.cosine_similarity(image_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=-1)\n",
    "\n",
    "    # 출력\n",
    "    print(\"image_embeds shape:\", image_embeds.shape)\n",
    "    print(\"text_embeds shape :\", text_embeds.shape)\n",
    "    print(\"\\n\\U0001F4CA Cosine Similarity Matrix:\\n\")\n",
    "    for i, img_id in enumerate(img_ids):\n",
    "        print(f\"\\U0001F5BC️ {img_id:012d}.jpg\")\n",
    "        for j, cap in enumerate(captions):\n",
    "            print(f\"  \\\"{cap}\\\" → similarity: {sim_matrix[i, j]:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_embeds shape: torch.Size([3, 512])\n",
      "text_embeds shape : torch.Size([3, 512])\n",
      "\n",
      "📊 Cosine Similarity Matrix:\n",
      "\n",
      "🖼️ 000000185472.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" → similarity: 0.2968\n",
      "  \"A filtered image of a microwave available to use in a store. \" → similarity: -0.1456\n",
      "  \"A fat orange cat on a couch beside a TV remote\" → similarity: -0.1866\n",
      "\n",
      "🖼️ 000000222455.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" → similarity: -0.1226\n",
      "  \"A filtered image of a microwave available to use in a store. \" → similarity: 0.7122\n",
      "  \"A fat orange cat on a couch beside a TV remote\" → similarity: 0.0761\n",
      "\n",
      "🖼️ 000000271728.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" → similarity: -0.0907\n",
      "  \"A filtered image of a microwave available to use in a store. \" → similarity: 0.1085\n",
      "  \"A fat orange cat on a couch beside a TV remote\" → similarity: 0.6556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(\n",
    "    model,\n",
    "    val_image_dir=\"val2017\",\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    sample_size=3 #랜덤으로 이미지 세장\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.imgid2caption:\n",
    "                self.imgid2caption[img_id] = ann['caption']\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        caption = self.imgid2caption[img_id]\n",
    "\n",
    "        encoding = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# val_dataset = SimpleCocoCaptionDataset(\n",
    "#     caption_json_path=\"annotations/captions_val2017.json\",\n",
    "#     image_root=\"val2017\",\n",
    "#     transform=transform,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답,오답쌍 평균 유사도 계산 추론\n",
    "def evaluate_mean_similarity(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct_sim = 0.0\n",
    "    total_incorrect_sim = 0.0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)  # (B, 512)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(\n",
    "                image_feat.unsqueeze(1),  # (B, 1, D)\n",
    "                text_feat.unsqueeze(0),  # (1, B, D)\n",
    "                dim=-1\n",
    "            )  # (B, B)\n",
    "\n",
    "            B = sim_matrix.size(0)\n",
    "            correct_sims = sim_matrix.diag()\n",
    "            total_correct_sim += correct_sims.sum().item()\n",
    "            correct_count += B\n",
    "\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "            incorrect_sims = sim_matrix[mask]\n",
    "            total_incorrect_sim += incorrect_sims.sum().item()\n",
    "            incorrect_count += incorrect_sims.numel()\n",
    "\n",
    "    mean_correct = total_correct_sim / correct_count\n",
    "    mean_incorrect = total_incorrect_sim / incorrect_count\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\" - Mean Correct Sim    : {mean_correct:.4f}\")\n",
    "    print(f\" - Mean Incorrect Sim  : {mean_incorrect:.4f}\")\n",
    "    return mean_correct, mean_incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 157/157 [08:47<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      " - Mean Correct Sim    : 0.6390\n",
      " - Mean Incorrect Sim  : 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_correct, mean_incorrect = evaluate_mean_similarity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후기단계 시작\n"
     ]
    }
   ],
   "source": [
    "print(\"후기단계 시작\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn_scalar, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn_scalar: float\n",
    "    curr_attn: [B, T, P]\n",
    "    \"\"\"\n",
    "    curr_mean = torch.clamp(curr_attn, min=eps).mean().item()\n",
    "    return abs(prev_attn_scalar - curr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(prev_ema, new_val, decay=0.9):\n",
    "    \"\"\"\n",
    "    new_val: [B, T, P] → scalar\n",
    "    prev_ema: scalar\n",
    "    \"\"\"\n",
    "    new_val_mean = new_val.mean().item()  # float\n",
    "    if prev_ema is None:\n",
    "        return new_val_mean\n",
    "    return decay * prev_ema + (1 - decay) * new_val_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Contrastive Loss ===\n",
    "# def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "#     image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "#     text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "#     logits = image_embeds @ text_embeds.T / temperature\n",
    "#     labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "#     loss_i2t = F.cross_entropy(logits, labels)\n",
    "#     loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "#     return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# # === Consistency Loss ===\n",
    "# def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "#     B, T, H, W = masks.shape\n",
    "#     masks_flat = masks.view(B, T, -1)\n",
    "#     scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "#     scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "#     return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # === 1. EMA 기반 안정 시점 추정 함수 ===\n",
    "# def train_model_with_ema(model, dataloader, optimizer, device, lambda_cons=0.05, ema_decay=0.9, num_epochs=10, std_threshold=0.01):\n",
    "#     model.train()\n",
    "#     ema_attn_scalar = None\n",
    "#     ema_scores_history = []\n",
    "#     stable_epoch = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss, total_acc = 0, 0\n",
    "#         progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "#         ema_epoch_scores = []\n",
    "        \n",
    "#         for images, captions, masks in progress:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "#             img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "#             loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "#             T_mask = masks.shape[1]\n",
    "            \n",
    "#             loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "#             curr_attn_mean = attn_weights.mean().item()\n",
    "#             ema_epoch_scores.append(curr_attn_mean)\n",
    "#             loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "#                 pred = sim_matrix.argmax(dim=1)\n",
    "#                 labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "#                 acc = (pred == labels).float().mean().item()\n",
    "                \n",
    "#             total_loss += loss.item()\n",
    "#             total_acc += acc\n",
    "#             progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "            \n",
    "#         if ema_attn_scalar is None:\n",
    "#             ema_attn_scalar = np.mean(ema_epoch_scores)\n",
    "            \n",
    "#         else:\n",
    "#             ema_attn_scalar = ema_decay * ema_attn_scalar + (1 - ema_decay) * np.mean(ema_epoch_scores)\n",
    "#         ema_scores_history.append(ema_attn_scalar)\n",
    "        \n",
    "#         if len(ema_scores_history) > 3:\n",
    "#             ema_scores_history.pop(0)\n",
    "            \n",
    "#         if len(ema_scores_history) == 3:\n",
    "#             std_ema = np.std(ema_scores_history)\n",
    "            \n",
    "#             if std_ema < std_threshold:\n",
    "#                 stable_epoch = epoch\n",
    "                \n",
    "#                 print(f\"[Warm-up 종료 기준 충족] Epoch: {epoch} (std_ema={std_ema:.6f})\")\n",
    "#         print(f\"[Epoch {epoch+1}] Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "#     print(f\"\\nEMA 기반 최종 Warm-up 종료 기준 (3회 std_ema < {std_threshold}): {stable_epoch}\")\n",
    "#     return stable_epoch\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "# def train_model_with_ema(model, dataloader, optimizer, device,\n",
    "#                          lambda_cons=0.05, ema_decay=0.9,\n",
    "#                          num_epochs=10, std_threshold=0.01):\n",
    "#     model.train()\n",
    "    \n",
    "#     ema_attn_scalar = None\n",
    "#     ema_scores_history = []\n",
    "#     stable_epoch = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss, total_acc = 0, 0\n",
    "#         progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "#         for images, captions, masks in progress:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "#             # 모델 forward\n",
    "#             img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "#             loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "#             T_mask = masks.shape[1]\n",
    "#             loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "#             loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "#             # Optimizer update\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Cosine similarity accuracy 계산\n",
    "#             with torch.no_grad():\n",
    "#                 sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "#                 pred = sim_matrix.argmax(dim=1)\n",
    "#                 labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "#                 acc = (pred == labels).float().mean().item()\n",
    "                \n",
    "#             total_loss += loss.item()\n",
    "#             total_acc += acc\n",
    "#             progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "            \n",
    "#             # === EMA 업데이트 (배치 단위로!) ===\n",
    "#             curr_attn_mean = attn_weights.mean().item()\n",
    "#             if ema_attn_scalar is None:\n",
    "#                 ema_attn_scalar = curr_attn_mean\n",
    "#             else:\n",
    "#                 ema_attn_scalar = ema_decay * ema_attn_scalar + (1 - ema_decay) * curr_attn_mean\n",
    "        \n",
    "#         # === Epoch마다 EMA 기록 및 안정화 판단 ===\n",
    "#         ema_scores_history.append(ema_attn_scalar)\n",
    "#         if len(ema_scores_history) > 3:\n",
    "#             ema_scores_history.pop(0)\n",
    "        \n",
    "#         if len(ema_scores_history) == 3:\n",
    "#             std_ema = np.std(ema_scores_history)\n",
    "#             if std_ema < std_threshold:\n",
    "#                 stable_epoch = epoch\n",
    "#                 print(f\"[Warm-up 종료 기준 충족] Epoch: {epoch} (std_ema={std_ema:.6f})\")\n",
    "        \n",
    "#         # Epoch별 평균 로그\n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         avg_acc = total_acc / len(dataloader)\n",
    "#         print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nEMA 기반 최종 Warm-up 종료 기준 (3회 std_ema < {std_threshold}): {stable_epoch}\")\n",
    "#     return stable_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_with_best_epoch(model, dataloader, optimizer, device,\n",
    "                                lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # 모델 forward\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            T_mask = masks.shape[1]\n",
    "            loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Cosine similarity accuracy 계산\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "        \n",
    "        # Epoch별 평균\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")\n",
    "        \n",
    "        # 가장 성능 좋은 epoch를 업데이트\n",
    "        if avg_loss < best_loss or (avg_loss == best_loss and avg_acc > best_acc):\n",
    "            best_loss = avg_loss\n",
    "            best_acc = avg_acc\n",
    "            best_epoch = epoch\n",
    "    \n",
    "    print(f\"\\n Best Warm-up Epoch: {best_epoch} (Loss: {best_loss:.4f}, Acc: {best_acc:.4f})\")\n",
    "    return best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.ema = None\n",
    "\n",
    "    def update(self, value):\n",
    "        with torch.no_grad():\n",
    "            v = value.mean().item()\n",
    "            if self.ema is None:\n",
    "                self.ema = v\n",
    "            else:\n",
    "                self.ema = self.alpha * v + (1 - self.alpha) * self.ema\n",
    "        return self.ema\n",
    "\n",
    "    def reset(self):\n",
    "        self.ema = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device,\n",
    "                lambda_self=0.1, warmup_epochs=3, num_epochs=5,\n",
    "                start_epoch=0, ema_tracker=None):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if ema_tracker is None:\n",
    "        ema_tracker = EMA(alpha=0.2)\n",
    "    use_ema = False\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\", leave=False)\n",
    "\n",
    "        if epoch == warmup_epochs:\n",
    "            ema_tracker.reset()\n",
    "            use_ema = True\n",
    "            print(f\"[Epoch {epoch}] EMA 기준 self-consistency 시작\")\n",
    "\n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :]\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            if epoch < warmup_epochs:\n",
    "                lambda_cons, _ = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "                loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "                loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "            else:\n",
    "                ema_attn = ema_tracker.update(attn_weights_matched)\n",
    "                loss_self_cons = kl_divergence_attention(ema_attn, attn_weights_matched.detach())\n",
    "                loss = loss_contrastive + lambda_self * loss_self_cons\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "\n",
    "    return ema_tracker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. 전체 실행\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Step 1: attention 안정 시점 추정\n",
    "# stable_epoch = train_model_with_ema(\n",
    "#     model, dataloader, optimizer, device,\n",
    "#     lambda_cons=0.05,\n",
    "#     ema_decay=0.9,\n",
    "#     num_epochs=10\n",
    "# )\n",
    "\n",
    "# # Step 2: 이후 self-consistency 중심 학습\n",
    "# ema_tracker = train_model(\n",
    "#     model, dataloader, optimizer, device,\n",
    "#     lambda_self=0.1,\n",
    "#     warmup_epochs=stable_epoch,\n",
    "#     num_epochs=10,\n",
    "#     start_epoch=stable_epoch\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss: 0.9357, Avg Acc: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Loss: 0.7615, Avg Acc: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Loss: 0.7248, Avg Acc: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Loss: 0.7193, Avg Acc: 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Loss: 0.7178, Avg Acc: 0.9670\n",
      "\n",
      "✅ Best Warm-up Epoch: 4 (Loss: 0.7178, Acc: 0.9670)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 5/14:   0%|                                      | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] EMA 기준 self-consistency 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.0916, Avg Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/14:  99%|███▉| 1240/1250 [24:14<00:09,  1.05it/s, loss=0.013, acc=1.000]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Warm-up 단계\n",
    "stable_epoch = train_model_with_best_epoch(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.05,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "# 이후 단계 (Self-consistency)\n",
    "ema_tracker = train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=stable_epoch,\n",
    "    num_epochs=10,\n",
    "    start_epoch=stable_epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wasp)",
   "language": "python",
   "name": "wasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
