{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val ['000000433103.jpg', '000000129113.jpg', '000000196843.jpg', '000000252507.jpg', '000000258541.jpg']\n",
      "train ['000000427548.jpg', '000000367442.jpg', '000000574946.jpg', '000000215255.jpg', '000000016119.jpg']\n",
      "valÌååÏùº Í∞úÏàò: 5000\n",
      "trainÌååÏùº Í∞úÏàò: 109933\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "val = \"val2017\"\n",
    "train_path = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "val_path = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "# folder_path = \"/raid/kyscap251/team2/val2017/val2017\"\n",
    "test = \"train2017\"\n",
    "\n",
    "val_items = os.listdir(val_path)\n",
    "train_items = os.listdir(train_path)\n",
    "\n",
    "print(\"val\", val_items[:5])\n",
    "print(\"train\", train_items[:5])\n",
    "\n",
    "# ÌååÏùºÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "files = [f for f in os.listdir(val_path)\n",
    "         if os.path.isfile(os.path.join(val_path, f))]\n",
    "\n",
    "print(f\"valÌååÏùº Í∞úÏàò: {len(files)}\")\n",
    "\n",
    "filess = [ff for ff in os.listdir(train_path)\n",
    "         if os.path.isfile(os.path.join(train_path, ff))]\n",
    "print(f\"trainÌååÏùº Í∞úÏàò: {len(filess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎî©\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ïù¥ÎØ∏ÏßÄ Ïù∏ÏΩîÎçî: ViT\n",
    "# class VisionEncoder(nn.Module):\n",
    "#     def __init__(self, output_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.vit = models.vit_b_16(pretrained=True)\n",
    "#         self.vit.heads = nn.Identity()  # classification head Ï†úÍ±∞\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         patch_feats = self.vit(images)  # [B, D]\n",
    "#         return patch_feats\n",
    "    \n",
    "\n",
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎçî: BERT Í∏∞Î∞ò Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Î™®Îç∏\n",
    "# class VisionLanguageModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.vision_encoder = VisionEncoder()\n",
    "#         self.text_encoder = TextEncoder()\n",
    "#         self.cross_attn = CrossAttentionBlock()\n",
    "#         self.proj_image = nn.Linear(768, 512)\n",
    "#         self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "#     def forward(self, images, captions):\n",
    "#         img_feat = self.vision_encoder(images)\n",
    "#         text_emb, tokens = self.text_encoder(captions)\n",
    "#         patch_feat = img_feat.unsqueeze(1)  # dummy patch feature (B, 1, D)\n",
    "#         cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)\n",
    "#         img_proj = self.proj_image(img_feat)\n",
    "#         text_proj = self.proj_text(text_emb[:, 0])  # CLS token Í∏∞Ï§Ä\n",
    "#         return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "#     def encode_for_inference(self, images, captions):\n",
    "#         with torch.no_grad():\n",
    "#             img_feat = self.vision_encoder(images)\n",
    "#             text_emb, _ = self.text_encoder(captions)\n",
    "#             img_proj = self.proj_image(img_feat)\n",
    "#             text_proj = self.proj_text(text_emb[:, 0])  # CLS token Í∏∞Ï§Ä\n",
    "#         return img_proj, text_proj\n",
    "    \n",
    "# -------------------------------\n",
    "# Vision-Language Model\n",
    "# -------------------------------\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #ÎûúÎç§ 3Í∞ú Ïú†ÏÇ¨ÎèÑ ÌèâÍ∞ÄÏö©\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ ÌèâÍ∞ÄÏö©\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS tokenÎßå Ï∂îÏ∂ú (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨Îêú JSON Î°úÎî© Î∞è binary mask ÏÉùÏÑ±\n",
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.data = []\n",
    "        for entry in all_data:\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"Ïú†Ìö® Ïù¥ÎØ∏ÏßÄ Ïàò: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            print(f\"[WARN] Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞ Ïã§Ìå®: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Îã§Ïùå Ïù∏Îç±Ïä§Î°ú Ïû¨ÏãúÎèÑ\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        masks = torch.zeros((self.max_tokens, H, W))\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            masks[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "        return image, caption, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    captions = [item[1] for item in batch]\n",
    "    masks = torch.stack([item[2] for item in batch])\n",
    "    return images, captions, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïú†Ìö® Ïù¥ÎØ∏ÏßÄ Ïàò: 5000\n"
     ]
    }
   ],
   "source": [
    "# DataLoader ÏÉùÏÑ±\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=\"val_coco_token_bbox_matched.json\",\n",
    "#    image_root=\"/raid/kyscap251/team2/val2017/val2017\",\n",
    "    image_root = \"/shared/home/kyscap251/Team2/val2017\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=coco_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #ÎûåÎã§ Ï†êÏßÑÏ†Å ÎÜíÏù¥Í∏∞\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤¥ Epoch ÌïôÏäµ Î£®ÌîÑ (Matched TokenÎßå ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏàòÏ†ï)\n",
    "def train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "#     ////////////\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "#     ////////////////////\n",
    "#     lambda_log = [] #Œª Î≥ÄÌôî Í∏∞Î°ùÏö© Î¶¨Ïä§Ìä∏\n",
    "#     diff_log = [] #diffÍ∞í Í∏∞Î°ùÏö© Î¶¨Ïä§Ìä∏\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            # matched token ÏàòÎßåÌÅº attention weight Ïä¨ÎùºÏù¥Ïã±\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :] #bboxÏóê Ìï¥ÎãπÌïòÎäî tokenÎßå consistency loss Í≥ÑÏÇ∞Ïóê ÏÇ¨Ïö©\n",
    "\n",
    "            \n",
    "#             //////////////\n",
    "            # ÌòÑÏû¨ Œª Í∞íÏùÑ EMA Í∏∞Î∞òÏúºÎ°ú Í≥ÑÏÇ∞ Î∞è Í∏∞Î°ù\n",
    "            lambda_cons, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "#             lambda_log.append(lambda_cons)\n",
    "#             diff_log.append(diff)\n",
    "# /////////////////////////////////////\n",
    "            \n",
    "            # ÏÜêÏã§ Í≥ÑÏÇ∞\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # cosine similarity + accuracy Í≥ÑÏÇ∞\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                sims = torch.diag(sim_matrix)\n",
    "                sim_mean = sims.mean().item()\n",
    "                sim_std = sims.std().item()\n",
    "\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"cos_sim\": f\"{sim_mean:.3f}¬±{sim_std:.3f}\", \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "#         # scheduler ÏóÖÎç∞Ïù¥Ìä∏\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "#             print(f\"[Epoch {epoch+1}] LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Î™®Îç∏ ÌïôÏäµ Ïã§Ìñâ\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎûúÎç§ 3Í∞ú Ï∂îÎ°†\n",
    "def run_batch_inference(model, val_image_dir, caption_json_path, transform, device, sample_size=3):\n",
    "    with open(caption_json_path, 'r') as f:\n",
    "        coco_captions = json.load(f)\n",
    "\n",
    "    # image_id ‚Üí caption Îß§Ìïë\n",
    "    imgid2caption = {}\n",
    "    for ann in coco_captions['annotations']:\n",
    "        imgid = ann['image_id']\n",
    "        if imgid not in imgid2caption:\n",
    "            imgid2caption[imgid] = []\n",
    "        imgid2caption[imgid].append(ann['caption'])\n",
    "\n",
    "    # ÎûúÎç§ ÏÉòÌîåÎßÅ (image_id 3Í∞ú)\n",
    "    img_ids = random.sample(list(imgid2caption.keys()), sample_size)\n",
    "    captions = [imgid2caption[i][0] for i in img_ids]\n",
    "    image_paths = [os.path.join(val_image_dir, f\"{i:012d}.jpg\") for i in img_ids]\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "    ]).to(device)\n",
    "\n",
    "    # Ïù∏ÏΩîÎî©\n",
    "    model.eval()\n",
    "    image_embeds, text_embeds = model.encode_for_inference(images_tensor, captions)\n",
    "\n",
    "    # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ\n",
    "    sim_matrix = F.cosine_similarity(image_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=-1)\n",
    "\n",
    "    # Ï∂úÎ†•\n",
    "    print(\"image_embeds shape:\", image_embeds.shape)\n",
    "    print(\"text_embeds shape :\", text_embeds.shape)\n",
    "    print(\"\\n\\U0001F4CA Cosine Similarity Matrix:\\n\")\n",
    "    for i, img_id in enumerate(img_ids):\n",
    "        print(f\"\\U0001F5BCÔ∏è {img_id:012d}.jpg\")\n",
    "        for j, cap in enumerate(captions):\n",
    "            print(f\"  \\\"{cap}\\\" ‚Üí similarity: {sim_matrix[i, j]:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_embeds shape: torch.Size([3, 512])\n",
      "text_embeds shape : torch.Size([3, 512])\n",
      "\n",
      "üìä Cosine Similarity Matrix:\n",
      "\n",
      "üñºÔ∏è 000000185472.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" ‚Üí similarity: 0.2968\n",
      "  \"A filtered image of a microwave available to use in a store. \" ‚Üí similarity: -0.1456\n",
      "  \"A fat orange cat on a couch beside a TV remote\" ‚Üí similarity: -0.1866\n",
      "\n",
      "üñºÔ∏è 000000222455.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" ‚Üí similarity: -0.1226\n",
      "  \"A filtered image of a microwave available to use in a store. \" ‚Üí similarity: 0.7122\n",
      "  \"A fat orange cat on a couch beside a TV remote\" ‚Üí similarity: 0.0761\n",
      "\n",
      "üñºÔ∏è 000000271728.jpg\n",
      "  \"The train is approaching and a man is getting off his bicycle.\" ‚Üí similarity: -0.0907\n",
      "  \"A filtered image of a microwave available to use in a store. \" ‚Üí similarity: 0.1085\n",
      "  \"A fat orange cat on a couch beside a TV remote\" ‚Üí similarity: 0.6556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_batch_inference(\n",
    "    model,\n",
    "    val_image_dir=\"val2017\",\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    sample_size=3 #ÎûúÎç§ÏúºÎ°ú Ïù¥ÎØ∏ÏßÄ ÏÑ∏Ïû•\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.imgid2caption:\n",
    "                self.imgid2caption[img_id] = ann['caption']\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        caption = self.imgid2caption[img_id]\n",
    "\n",
    "        encoding = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# val_dataset = SimpleCocoCaptionDataset(\n",
    "#     caption_json_path=\"annotations/captions_val2017.json\",\n",
    "#     image_root=\"val2017\",\n",
    "#     transform=transform,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÎãµ,Ïò§ÎãµÏåç ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ï∂îÎ°†\n",
    "def evaluate_mean_similarity(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct_sim = 0.0\n",
    "    total_incorrect_sim = 0.0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)  # (B, 512)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(\n",
    "                image_feat.unsqueeze(1),  # (B, 1, D)\n",
    "                text_feat.unsqueeze(0),  # (1, B, D)\n",
    "                dim=-1\n",
    "            )  # (B, B)\n",
    "\n",
    "            B = sim_matrix.size(0)\n",
    "            correct_sims = sim_matrix.diag()\n",
    "            total_correct_sim += correct_sims.sum().item()\n",
    "            correct_count += B\n",
    "\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "            incorrect_sims = sim_matrix[mask]\n",
    "            total_incorrect_sim += incorrect_sims.sum().item()\n",
    "            incorrect_count += incorrect_sims.numel()\n",
    "\n",
    "    mean_correct = total_correct_sim / correct_count\n",
    "    mean_incorrect = total_incorrect_sim / incorrect_count\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\" - Mean Correct Sim    : {mean_correct:.4f}\")\n",
    "    print(f\" - Mean Incorrect Sim  : {mean_incorrect:.4f}\")\n",
    "    return mean_correct, mean_incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [08:47<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      " - Mean Correct Sim    : 0.6390\n",
      " - Mean Incorrect Sim  : 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_correct, mean_incorrect = evaluate_mean_similarity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌõÑÍ∏∞Îã®Í≥Ñ ÏãúÏûë\n"
     ]
    }
   ],
   "source": [
    "print(\"ÌõÑÍ∏∞Îã®Í≥Ñ ÏãúÏûë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn_scalar, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn_scalar: float\n",
    "    curr_attn: [B, T, P]\n",
    "    \"\"\"\n",
    "    curr_mean = torch.clamp(curr_attn, min=eps).mean().item()\n",
    "    return abs(prev_attn_scalar - curr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(prev_ema, new_val, decay=0.9):\n",
    "    \"\"\"\n",
    "    new_val: [B, T, P] ‚Üí scalar\n",
    "    prev_ema: scalar\n",
    "    \"\"\"\n",
    "    new_val_mean = new_val.mean().item()  # float\n",
    "    if prev_ema is None:\n",
    "        return new_val_mean\n",
    "    return decay * prev_ema + (1 - decay) * new_val_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Contrastive Loss ===\n",
    "# def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "#     image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "#     text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "#     logits = image_embeds @ text_embeds.T / temperature\n",
    "#     labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "#     loss_i2t = F.cross_entropy(logits, labels)\n",
    "#     loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "#     return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# # === Consistency Loss ===\n",
    "# def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "#     B, T, H, W = masks.shape\n",
    "#     masks_flat = masks.view(B, T, -1)\n",
    "#     scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "#     scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "#     return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # === 1. EMA Í∏∞Î∞ò ÏïàÏ†ï ÏãúÏ†ê Ï∂îÏ†ï Ìï®Ïàò ===\n",
    "# def train_model_with_ema(model, dataloader, optimizer, device, lambda_cons=0.05, ema_decay=0.9, num_epochs=10, std_threshold=0.01):\n",
    "#     model.train()\n",
    "#     ema_attn_scalar = None\n",
    "#     ema_scores_history = []\n",
    "#     stable_epoch = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss, total_acc = 0, 0\n",
    "#         progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "#         ema_epoch_scores = []\n",
    "        \n",
    "#         for images, captions, masks in progress:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "#             img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "#             loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "#             T_mask = masks.shape[1]\n",
    "            \n",
    "#             loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "#             curr_attn_mean = attn_weights.mean().item()\n",
    "#             ema_epoch_scores.append(curr_attn_mean)\n",
    "#             loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "#                 pred = sim_matrix.argmax(dim=1)\n",
    "#                 labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "#                 acc = (pred == labels).float().mean().item()\n",
    "                \n",
    "#             total_loss += loss.item()\n",
    "#             total_acc += acc\n",
    "#             progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "            \n",
    "#         if ema_attn_scalar is None:\n",
    "#             ema_attn_scalar = np.mean(ema_epoch_scores)\n",
    "            \n",
    "#         else:\n",
    "#             ema_attn_scalar = ema_decay * ema_attn_scalar + (1 - ema_decay) * np.mean(ema_epoch_scores)\n",
    "#         ema_scores_history.append(ema_attn_scalar)\n",
    "        \n",
    "#         if len(ema_scores_history) > 3:\n",
    "#             ema_scores_history.pop(0)\n",
    "            \n",
    "#         if len(ema_scores_history) == 3:\n",
    "#             std_ema = np.std(ema_scores_history)\n",
    "            \n",
    "#             if std_ema < std_threshold:\n",
    "#                 stable_epoch = epoch\n",
    "                \n",
    "#                 print(f\"[Warm-up Ï¢ÖÎ£å Í∏∞Ï§Ä Ï∂©Ï°±] Epoch: {epoch} (std_ema={std_ema:.6f})\")\n",
    "#         print(f\"[Epoch {epoch+1}] Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "#     print(f\"\\nEMA Í∏∞Î∞ò ÏµúÏ¢Ö Warm-up Ï¢ÖÎ£å Í∏∞Ï§Ä (3Ìöå std_ema < {std_threshold}): {stable_epoch}\")\n",
    "#     return stable_epoch\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "# def train_model_with_ema(model, dataloader, optimizer, device,\n",
    "#                          lambda_cons=0.05, ema_decay=0.9,\n",
    "#                          num_epochs=10, std_threshold=0.01):\n",
    "#     model.train()\n",
    "    \n",
    "#     ema_attn_scalar = None\n",
    "#     ema_scores_history = []\n",
    "#     stable_epoch = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss, total_acc = 0, 0\n",
    "#         progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "#         for images, captions, masks in progress:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "#             # Î™®Îç∏ forward\n",
    "#             img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "#             loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "#             T_mask = masks.shape[1]\n",
    "#             loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "#             loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "#             # Optimizer update\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Cosine similarity accuracy Í≥ÑÏÇ∞\n",
    "#             with torch.no_grad():\n",
    "#                 sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "#                 pred = sim_matrix.argmax(dim=1)\n",
    "#                 labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "#                 acc = (pred == labels).float().mean().item()\n",
    "                \n",
    "#             total_loss += loss.item()\n",
    "#             total_acc += acc\n",
    "#             progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "            \n",
    "#             # === EMA ÏóÖÎç∞Ïù¥Ìä∏ (Î∞∞Ïπò Îã®ÏúÑÎ°ú!) ===\n",
    "#             curr_attn_mean = attn_weights.mean().item()\n",
    "#             if ema_attn_scalar is None:\n",
    "#                 ema_attn_scalar = curr_attn_mean\n",
    "#             else:\n",
    "#                 ema_attn_scalar = ema_decay * ema_attn_scalar + (1 - ema_decay) * curr_attn_mean\n",
    "        \n",
    "#         # === EpochÎßàÎã§ EMA Í∏∞Î°ù Î∞è ÏïàÏ†ïÌôî ÌåêÎã® ===\n",
    "#         ema_scores_history.append(ema_attn_scalar)\n",
    "#         if len(ema_scores_history) > 3:\n",
    "#             ema_scores_history.pop(0)\n",
    "        \n",
    "#         if len(ema_scores_history) == 3:\n",
    "#             std_ema = np.std(ema_scores_history)\n",
    "#             if std_ema < std_threshold:\n",
    "#                 stable_epoch = epoch\n",
    "#                 print(f\"[Warm-up Ï¢ÖÎ£å Í∏∞Ï§Ä Ï∂©Ï°±] Epoch: {epoch} (std_ema={std_ema:.6f})\")\n",
    "        \n",
    "#         # EpochÎ≥Ñ ÌèâÍ∑† Î°úÍ∑∏\n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         avg_acc = total_acc / len(dataloader)\n",
    "#         print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nEMA Í∏∞Î∞ò ÏµúÏ¢Ö Warm-up Ï¢ÖÎ£å Í∏∞Ï§Ä (3Ìöå std_ema < {std_threshold}): {stable_epoch}\")\n",
    "#     return stable_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_with_best_epoch(model, dataloader, optimizer, device,\n",
    "                                lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Î™®Îç∏ forward\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            T_mask = masks.shape[1]\n",
    "            loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_bbox_cons\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Cosine similarity accuracy Í≥ÑÏÇ∞\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "        \n",
    "        # EpochÎ≥Ñ ÌèâÍ∑†\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")\n",
    "        \n",
    "        # Í∞ÄÏû• ÏÑ±Îä• Ï¢ãÏùÄ epochÎ•º ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        if avg_loss < best_loss or (avg_loss == best_loss and avg_acc > best_acc):\n",
    "            best_loss = avg_loss\n",
    "            best_acc = avg_acc\n",
    "            best_epoch = epoch\n",
    "    \n",
    "    print(f\"\\n Best Warm-up Epoch: {best_epoch} (Loss: {best_loss:.4f}, Acc: {best_acc:.4f})\")\n",
    "    return best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.ema = None\n",
    "\n",
    "    def update(self, value):\n",
    "        with torch.no_grad():\n",
    "            v = value.mean().item()\n",
    "            if self.ema is None:\n",
    "                self.ema = v\n",
    "            else:\n",
    "                self.ema = self.alpha * v + (1 - self.alpha) * self.ema\n",
    "        return self.ema\n",
    "\n",
    "    def reset(self):\n",
    "        self.ema = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device,\n",
    "                lambda_self=0.1, warmup_epochs=3, num_epochs=5,\n",
    "                start_epoch=0, ema_tracker=None):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if ema_tracker is None:\n",
    "        ema_tracker = EMA(alpha=0.2)\n",
    "    use_ema = False\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\", leave=False)\n",
    "\n",
    "        if epoch == warmup_epochs:\n",
    "            ema_tracker.reset()\n",
    "            use_ema = True\n",
    "            print(f\"[Epoch {epoch}] EMA Í∏∞Ï§Ä self-consistency ÏãúÏûë\")\n",
    "\n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :]\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            if epoch < warmup_epochs:\n",
    "                lambda_cons, _ = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "                loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "                loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "            else:\n",
    "                ema_attn = ema_tracker.update(attn_weights_matched)\n",
    "                loss_self_cons = kl_divergence_attention(ema_attn, attn_weights_matched.detach())\n",
    "                loss = loss_contrastive + lambda_self * loss_self_cons\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "\n",
    "    return ema_tracker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Ï†ÑÏ≤¥ Ïã§Ìñâ\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Step 1: attention ÏïàÏ†ï ÏãúÏ†ê Ï∂îÏ†ï\n",
    "# stable_epoch = train_model_with_ema(\n",
    "#     model, dataloader, optimizer, device,\n",
    "#     lambda_cons=0.05,\n",
    "#     ema_decay=0.9,\n",
    "#     num_epochs=10\n",
    "# )\n",
    "\n",
    "# # Step 2: Ïù¥ÌõÑ self-consistency Ï§ëÏã¨ ÌïôÏäµ\n",
    "# ema_tracker = train_model(\n",
    "#     model, dataloader, optimizer, device,\n",
    "#     lambda_self=0.1,\n",
    "#     warmup_epochs=stable_epoch,\n",
    "#     num_epochs=10,\n",
    "#     start_epoch=stable_epoch\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss: 0.9357, Avg Acc: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Loss: 0.7615, Avg Acc: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Loss: 0.7248, Avg Acc: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Loss: 0.7193, Avg Acc: 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Loss: 0.7178, Avg Acc: 0.9670\n",
      "\n",
      "‚úÖ Best Warm-up Epoch: 4 (Loss: 0.7178, Acc: 0.9670)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 5/14:   0%|                                      | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] EMA Í∏∞Ï§Ä self-consistency ÏãúÏûë\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.0916, Avg Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/14:  99%|‚ñà‚ñà‚ñà‚ñâ| 1240/1250 [24:14<00:09,  1.05it/s, loss=0.013, acc=1.000]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Warm-up Îã®Í≥Ñ\n",
    "stable_epoch = train_model_with_best_epoch(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.05,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "# Ïù¥ÌõÑ Îã®Í≥Ñ (Self-consistency)\n",
    "ema_tracker = train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=stable_epoch,\n",
    "    num_epochs=10,\n",
    "    start_epoch=stable_epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wasp)",
   "language": "python",
   "name": "wasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
