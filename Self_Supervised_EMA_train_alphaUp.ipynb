{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 상승 기반 warmup 종료시점 판단 적용(EMA랑 다름)\n",
    "# warm up, self-supervised분리\n",
    "# diff가 크면 alpha를 커지는 식의 EMA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정 및 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "from collections import Counter\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# 이미지 인코더: Vision Transformer 기반\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 인코더: BERT 기반 Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-Language Model\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #랜덤 3개 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #평균 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS token만 추출 (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss 계산 함수\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "json_path = \"coco_token_bbox_matched.json\"\n",
    "image_root = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "\n",
    "# 1. Load JSON once\n",
    "with open(json_path, \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# 2. 유효 이미지와 대표 label 추출\n",
    "valid_indices = []\n",
    "labels = []\n",
    "\n",
    "def get_dominant_label(matches):\n",
    "    if not matches:\n",
    "        return \"none\"\n",
    "    return Counter([m[\"label\"] for m in matches]).most_common(1)[0][0]\n",
    "\n",
    "for i, entry in enumerate(all_data):\n",
    "    image_id = entry[\"image_id\"]\n",
    "    image_path = os.path.join(image_root, f\"{image_id:012d}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        valid_indices.append(i)\n",
    "        labels.append(get_dominant_label(entry[\"matches\"]))\n",
    "\n",
    "# 3. stratified split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
    "subset_rel_idx, _ = next(splitter.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "# 4. 전체 JSON 기준으로 실제 subset 인덱스로 변환\n",
    "subset_json_indices = [valid_indices[i] for i in subset_rel_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "json_path = \"coco_token_bbox_matched.json\"\n",
    "image_root = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "\n",
    "# 1. Load JSON once\n",
    "with open(json_path, \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# 2. 유효 이미지와 대표 label 추출\n",
    "valid_indices = []\n",
    "labels = []\n",
    "\n",
    "def get_dominant_label(matches):\n",
    "    if not matches:\n",
    "        return \"none\"\n",
    "    return Counter([m[\"label\"] for m in matches]).most_common(1)[0][0]\n",
    "\n",
    "for i, entry in enumerate(all_data):\n",
    "    image_id = entry[\"image_id\"]\n",
    "    image_path = os.path.join(image_root, f\"{image_id:012d}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        valid_indices.append(i)\n",
    "        labels.append(get_dominant_label(entry[\"matches\"]))\n",
    "\n",
    "# 3. stratified split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
    "subset_rel_idx, _ = next(splitter.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "# 4. 전체 JSON 기준으로 실제 subset 인덱스로 변환\n",
    "subset_json_indices = [valid_indices[i] for i in subset_rel_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10, subset_indices=None):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.data = []\n",
    "        selected = subset_indices if subset_indices is not None else range(len(all_data))\n",
    "\n",
    "        for i in selected:\n",
    "            entry = all_data[i]\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"유효 이미지 수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            # print(f\"[WARN] 이미지 불러오기 실패: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # 다음 인덱스로 재시도\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            image_tensor = image\n",
    "\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "\n",
    "        # Tokenize caption\n",
    "        caption_tokens = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                        truncation=True, max_length=30)[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # Binary mask 생성\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        mask_tensor = torch.zeros((self.max_tokens, H, W))\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            mask_tensor[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "\n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'caption': caption,\n",
    "            'mask': mask_tensor,\n",
    "            'image_id': image_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch]  # ← 텐서 아님, 그냥 리스트로\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    return images, captions, masks, image_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효 이미지 수: 54966\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=json_path,\n",
    "    image_root=image_root,\n",
    "    transform=transform,\n",
    "    subset_indices=subset_json_indices\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=coco_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #람다 점진적 높이기\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn, curr_attn: [B, T, P] - softmax된 attention map\n",
    "    \"\"\"\n",
    "    prev = torch.clamp(prev_attn, min=eps)\n",
    "    curr = torch.clamp(curr_attn, min=eps)\n",
    "    kl = (prev * (prev.log() - curr.log())).sum(dim=-1).mean()  # 평균 over T, B\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.ema = None\n",
    "\n",
    "    def update(self, value):\n",
    "        with torch.no_grad():\n",
    "            if self.ema is None:\n",
    "                self.ema = value.detach()\n",
    "            else:\n",
    "                self.ema = self.alpha * value.detach() + (1 - self.alpha) * self.ema\n",
    "        return self.ema\n",
    "\n",
    "    def reset(self):\n",
    "        self.ema = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(ema_model, model, alpha):\n",
    "    with torch.no_grad():\n",
    "        for ema_param, model_param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(1 - alpha).add_(model_param.data, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def train_warmup(model, dataloader, optimizer, device,\n",
    "                 lambda_cons=0.5, warmup_epochs=10, start_epoch=0):\n",
    "    model.train()\n",
    "    bbox_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.005, max_lambda=lambda_cons)\n",
    "\n",
    "    acc_history = []\n",
    "    prev_acc = None\n",
    "    drop_count = 0\n",
    "    high_point_epoch = None\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + warmup_epochs):\n",
    "        total_loss, total_acc = 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        curr_attn_dict = {}\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f\"Warm-up Epoch {epoch}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images, captions, masks, image_ids = batch\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_attn = attn_weights.size(1)\n",
    "            T_mask = masks.size(1)\n",
    "            T_common = min(T_attn, T_mask)\n",
    "\n",
    "            attn_weights_slice = attn_weights[:, :T_common, :]\n",
    "            masks_slice = masks[:, :T_common]\n",
    "\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            lambda_bbox, _ = bbox_scheduler.update(attn_weights_slice, return_diff=True)\n",
    "            loss_bbox_cons = compute_consistency_loss(attn_weights_slice, masks_slice)\n",
    "            loss_bbox_cons = torch.clamp(loss_bbox_cons, max=1.0)\n",
    "            loss = loss_contrastive + lambda_bbox * loss_bbox_cons\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # attention map 저장\n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                curr_attn_dict[str(img_id)] = attn_weights[i].detach().cpu()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "        acc_history.append(avg_acc)\n",
    "        print(f\"[WARM-UP] Epoch {epoch} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'attn_dict': curr_attn_dict},\n",
    "                   f\"temp_checkpoint_epoch{epoch}_cjy01.pth\")\n",
    "\n",
    "        if prev_acc is not None and prev_acc > avg_acc:\n",
    "            drop_count += 1\n",
    "        else:\n",
    "            drop_count = 0\n",
    "\n",
    "        prev_acc = avg_acc\n",
    "\n",
    "        if drop_count >= 1:\n",
    "            high_point_epoch = epoch - 1\n",
    "            print(f\"[WARM-UP 종료 감지] Accuracy 하락 → 고점 epoch: {high_point_epoch}\")\n",
    "            shutil.copyfile(f\"temp_checkpoint_epoch{high_point_epoch}_cjy01.pth\",\n",
    "                            f\"checkpoint_epoch{high_point_epoch}_stable_cjy01.pth\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warm-up Epoch 0:   0%|                                | 0/13742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 0 - Avg Loss: 0.6770, Avg Accuracy: 0.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 1 - Avg Loss: 0.6277, Avg Accuracy: 0.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 2 - Avg Loss: 0.6066, Avg Accuracy: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 3 - Avg Loss: 0.6115, Avg Accuracy: 0.9562\n",
      "[WARM-UP 종료 감지] Accuracy 하락 → 고점 epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# 기기 설정 및 초기화\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Warm-up 학습 함수 실행\n",
    "train_warmup(\n",
    "    model=model,\n",
    "    dataloader=dataloader,  # 사용 중인 dataloader 그대로 입력\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    lambda_cons=0.5,         # bbox consistency weight \n",
    "    warmup_epochs=10,       # 충분한 최대 epoch 설정\n",
    "    start_epoch=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_self_start_cjy03.pth'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(\"checkpoint_epoch2_stable_cjy01.pth\", \"checkpoint_self_start_cjy03.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_self_supervised_start(path=\"checkpoint_self_start_cjy03.pth\", lr=5e-5):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = VisionLanguageModel().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    prev_attn_dict = checkpoint['attn_dict']\n",
    "    return model, optimizer, prev_attn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_supervised(model, dataloader, optimizer, device,\n",
    "                          prev_attn_dict, lambda_self=0.1, num_epochs=10, start_epoch=0):\n",
    "    model.train()\n",
    "    acc_history = []\n",
    "    # LambdaScheduler 추가 (self-consistency용)\n",
    "    lambda_self_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=lambda_self)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc = 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        curr_attn_dict = {}\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f\"Self Epoch {epoch}\", leave=False)\n",
    "        for batch_idx, batch in enumerate(progress):\n",
    "            images, captions, _, image_ids = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "            T_common = attn_weights.size(1)\n",
    "            attn_weights_slice = attn_weights[:, :T_common, :]\n",
    "\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            loss_self = 0.0\n",
    "            count = 0\n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                # image_id 문자열로 변환\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                img_id_str = str(img_id)\n",
    "\n",
    "                if img_id_str in prev_attn_dict:\n",
    "                    prev_attn = prev_attn_dict[img_id_str].to(device)\n",
    "                    curr_attn = attn_weights[i]\n",
    "\n",
    "                    T_curr = curr_attn.size(0)\n",
    "                    T_prev = prev_attn.size(0)\n",
    "                    T_common = min(T_curr, T_prev)\n",
    "\n",
    "                    prev_attn_crop = prev_attn[:T_common, :]\n",
    "                    curr_attn_crop = curr_attn[:T_common, :]\n",
    "\n",
    "                    # EMA alpha 결정\n",
    "                    diff = F.mse_loss(curr_attn_crop, prev_attn_crop, reduction='mean').item()\n",
    "#                     print(f\"Current diff: {diff}\")\n",
    "                    alpha = 0.1 + 0.8 * min(diff, 1.0)  # diff↑ → alpha↑  # 차이 클수록 alpha 크게\n",
    "\n",
    "                    # EMA 적용\n",
    "                    updated_attn = alpha * curr_attn_crop + (1 - alpha) * prev_attn_crop\n",
    "\n",
    "                    # Self loss 계산은 EMA와 prev 기준\n",
    "                    loss_self += F.mse_loss(updated_attn, prev_attn_crop)\n",
    "\n",
    "                    # EMA된 attention 저장\n",
    "                    curr_attn_dict[img_id_str] = updated_attn.detach().cpu()\n",
    "                    count += 1\n",
    "\n",
    "            loss_self = loss_self / count if count > 0 else torch.tensor(0.0, device=device)\n",
    "            # LambdaScheduler로부터 가변 lambda_self 적용\n",
    "            lambda_self_val, diff = lambda_self_scheduler.update(attn_weights_slice, return_diff=True)\n",
    "            loss = loss_contrastive + lambda_self_val * loss_self\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "        acc_history.append(avg_acc)\n",
    "\n",
    "        print(f\"[SELF] Epoch {epoch} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"[SELF] Epoch {epoch} - Final Self Loss: {loss_self.item():.4f}, Final Match Count: {count}\")\n",
    "\n",
    "        torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'attn_dict': curr_attn_dict},\n",
    "                   f\"temp_checkpoint_self_epoch{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Self Epoch 1:   0%|  | 2/13742 [00:00<24:17,  9.43it/s, loss=7.94e-5, acc=1.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 1 - Avg Loss: 1.0983, Avg Accuracy: 0.4095\n",
      "[SELF] Epoch 1 - Final Self Loss: 0.0000, Final Match Count: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 2 - Avg Loss: 1.3863, Avg Accuracy: 0.2510\n",
      "[SELF] Epoch 2 - Final Self Loss: 0.0000, Final Match Count: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 3 - Avg Loss: 1.3863, Avg Accuracy: 0.2492\n",
      "[SELF] Epoch 3 - Final Self Loss: 0.0000, Final Match Count: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 4 - Avg Loss: 1.3866, Avg Accuracy: 0.2487\n",
      "[SELF] Epoch 4 - Final Self Loss: 0.0000, Final Match Count: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 5 - Avg Loss: 1.3864, Avg Accuracy: 0.2507\n",
      "[SELF] Epoch 5 - Final Self Loss: 0.0000, Final Match Count: 2\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "#  고점 체크포인트 불러오기\n",
    "model, optimizer, prev_attn_dict = load_self_supervised_start()\n",
    "\n",
    "ema_model = copy.deepcopy(model)\n",
    "\n",
    "#  self-consistency 학습용 train_model 함수 실행\n",
    "train_self_supervised(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    lambda_self=0.5,\n",
    "    num_epochs=5, # 원하는 만큼 self-consistency 학습 횟수\n",
    "    start_epoch=1,\n",
    "    prev_attn_dict=prev_attn_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.imgid2caption:\n",
    "                self.imgid2caption[img_id] = ann['caption']\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        caption = self.imgid2caption[img_id]\n",
    "\n",
    "        encoding = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "val_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답,오답쌍 평균 유사도 계산 추론\n",
    "def evaluate_mean_similarity(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct_sim = 0.0\n",
    "    total_incorrect_sim = 0.0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)  # (B, 512)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(\n",
    "                image_feat.unsqueeze(1),  # (B, 1, D)\n",
    "                text_feat.unsqueeze(0),  # (1, B, D)\n",
    "                dim=-1\n",
    "            )  # (B, B)\n",
    "\n",
    "            B = sim_matrix.size(0)\n",
    "            correct_sims = sim_matrix.diag()\n",
    "            total_correct_sim += correct_sims.sum().item()\n",
    "            correct_count += B\n",
    "\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "            incorrect_sims = sim_matrix[mask]\n",
    "            total_incorrect_sim += incorrect_sims.sum().item()\n",
    "            incorrect_count += incorrect_sims.numel()\n",
    "\n",
    "    mean_correct = total_correct_sim / correct_count\n",
    "    mean_incorrect = total_incorrect_sim / incorrect_count\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\" - Mean Correct Sim    : {mean_correct:.4f}\")\n",
    "    print(f\" - Mean Incorrect Sim  : {mean_incorrect:.4f}\")\n",
    "    return mean_correct, mean_incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [LambdaScheduler] Consistency loss ON (diff=0.000000)\n",
    "                                                                                \n",
    "# [SELF] Epoch 1 - Avg Loss: 0.1247, Avg Accuracy: 0.9518\n",
    "\n",
    "# [SELF] Epoch 2 - Avg Loss: 0.1134, Avg Accuracy: 0.9560\n",
    "\n",
    "# [SELF] Epoch 3 - Avg Loss: 0.1044, Avg Accuracy: 0.9589\n",
    "\n",
    "# [SELF] Epoch 4 - Avg Loss: 0.8424, Avg Accuracy: 0.5520\n",
    "\n",
    "# 이렇게 학습됐던 때에 돌린 결과. 에포크 3에서의 pth가 최종 모델이 되는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_checkpoint_self_cjy01.pth'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(\"temp_checkpoint_self_epoch3.pth\", \"end_checkpoint_self_cjy01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def load_model_for_evaluation(checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model = VisionLanguageModel().to(device)\n",
    "    model.load_state_dict(checkpoint['model'])  # student 모델 로드\n",
    "    # 또는 teacher 모델 사용 시: model.load_state_dict(checkpoint['ema_model'])\n",
    "    return model\n",
    "\n",
    "# 모델 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model_for_evaluation(\"end_checkpoint_self_cjy01.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 157/157 [09:44<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      " - Mean Correct Sim    : 0.6022\n",
      " - Mean Incorrect Sim  : 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_correct, mean_incorrect = evaluate_mean_similarity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def retrieval_evaluate(model, val_loader, device, max_samples=5000):\n",
    "    model.eval()\n",
    "    image_embeds_list = []\n",
    "    text_embeds_list = []\n",
    "    text_list = []\n",
    "    img_id_list = []\n",
    "\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Encoding image-text pairs\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            # (B, 512)\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)\n",
    "\n",
    "            image_embeds_list.append(image_feat)\n",
    "            text_embeds_list.append(text_feat)\n",
    "            n_samples += images.size(0)\n",
    "            if n_samples >= max_samples:\n",
    "                break\n",
    "\n",
    "    # (N, 512)\n",
    "    image_embeds_all = torch.cat(image_embeds_list, dim=0)[:max_samples]\n",
    "    text_embeds_all = torch.cat(text_embeds_list, dim=0)[:max_samples]\n",
    "\n",
    "    # 정규화\n",
    "    image_embeds_all = F.normalize(image_embeds_all, dim=-1)\n",
    "    text_embeds_all = F.normalize(text_embeds_all, dim=-1)\n",
    "\n",
    "    # 유사도 행렬 (N, N)\n",
    "    sim_matrix = torch.matmul(image_embeds_all, text_embeds_all.T)\n",
    "\n",
    "    def compute_recall(sim_matrix, k):\n",
    "        correct = 0\n",
    "        for i in range(sim_matrix.size(0)):\n",
    "            topk = sim_matrix[i].topk(k).indices\n",
    "            if i in topk:\n",
    "                correct += 1\n",
    "        return correct / sim_matrix.size(0)\n",
    "\n",
    "    recall1 = compute_recall(sim_matrix, 1)\n",
    "    recall5 = compute_recall(sim_matrix, 5)\n",
    "    recall10 = compute_recall(sim_matrix, 10)\n",
    "\n",
    "    print(\"\\n[Image-Text Retrieval Results (Closed-domain)]\")\n",
    "    print(f\"Recall@1: {recall1:.4f}\")\n",
    "    print(f\"Recall@5: {recall5:.4f}\")\n",
    "    print(f\"Recall@10: {recall10:.4f}\")\n",
    "\n",
    "    return recall1, recall5, recall10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding image-text pairs:  99%|█████████████▉| 156/157 [13:56<00:05,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Image-Text Retrieval Results (Closed-domain)]\n",
      "Recall@1: 0.0454\n",
      "Recall@5: 0.1540\n",
      "Recall@10: 0.2468\n"
     ]
    }
   ],
   "source": [
    "recall1, recall5, recall10 = retrieval_evaluate(model, val_loader, device, max_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP에서의 Retrieval Results 보기 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CocoClipEvaluationDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            self.imgid2caption[img_id] = ann['caption']\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        caption = self.imgid2caption[img_id]\n",
    "        return image, caption  # 2개 값만 반환\n",
    "\n",
    "\n",
    "\n",
    "def openclip_retrieval_evaluate(model, val_loader, tokenizer, device, max_samples=5000):\n",
    "    model.eval()\n",
    "    image_embeds_list = []\n",
    "    text_embeds_list = []\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 2개 값만 언패킹 (images, texts)\n",
    "        for images, texts in tqdm(val_loader, desc=\"Encoding image-text pairs\"):\n",
    "            images = images.to(device)\n",
    "            # 텍스트 토크나이징\n",
    "            text_tokens = tokenizer(texts).to(device)\n",
    "            \n",
    "            # 임베딩 추출\n",
    "            image_embeds = model.encode_image(images)\n",
    "            text_embeds = model.encode_text(text_tokens)\n",
    "            \n",
    "            image_embeds_list.append(image_embeds)\n",
    "            text_embeds_list.append(text_embeds)\n",
    "            n_samples += images.size(0)\n",
    "            if n_samples >= max_samples:\n",
    "                break\n",
    "\n",
    "    image_embeds_all = torch.cat(image_embeds_list, dim=0)[:max_samples]\n",
    "    text_embeds_all = torch.cat(text_embeds_list, dim=0)[:max_samples]\n",
    "\n",
    "    image_embeds_all = F.normalize(image_embeds_all, dim=-1)\n",
    "    text_embeds_all = F.normalize(text_embeds_all, dim=-1)\n",
    "\n",
    "    sim_matrix = torch.matmul(image_embeds_all, text_embeds_all.T)\n",
    "\n",
    "    def compute_recall(sim_matrix, k):\n",
    "        correct = 0\n",
    "        for i in range(sim_matrix.size(0)):\n",
    "            topk = sim_matrix[i].topk(k).indices\n",
    "            if i in topk:\n",
    "                correct += 1\n",
    "        return correct / sim_matrix.size(0)\n",
    "\n",
    "    recall1 = compute_recall(sim_matrix, 1)\n",
    "    recall5 = compute_recall(sim_matrix, 5)\n",
    "    recall10 = compute_recall(sim_matrix, 10)\n",
    "\n",
    "    print(\"\\n[Image-Text Retrieval Results (Closed-domain)]\")\n",
    "    print(f\"Recall@1: {recall1:.4f}\")\n",
    "    print(f\"Recall@5: {recall5:.4f}\")\n",
    "    print(f\"Recall@10: {recall10:.4f}\")\n",
    "\n",
    "    return recall1, recall5, recall10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 전용 transform\n",
    "_, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "clip_val_dataset = CocoClipEvaluationDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=preprocess_clip\n",
    ")\n",
    "\n",
    "clip_val_loader = DataLoader(\n",
    "    clip_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding image-text pairs:  99%|█████████████▉| 156/157 [13:27<00:05,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Image-Text Retrieval Results (Closed-domain)]\n",
      "Recall@1: 0.4108\n",
      "Recall@5: 0.6770\n",
      "Recall@10: 0.7716\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 초기화\n",
    "model_clip, _, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer_clip = open_clip.get_tokenizer('ViT-B-32')\n",
    "device_clip = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_clip = model_clip.to(device_clip)\n",
    "\n",
    "# 평가 실행\n",
    "recall1, recall5, recall10 = openclip_retrieval_evaluate(\n",
    "    model_clip,\n",
    "    clip_val_loader,  # 2개 값 반환하는 데이터로더\n",
    "    tokenizer_clip,\n",
    "    device_clip,\n",
    "    max_samples=5000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu118env)",
   "language": "python",
   "name": "cu118env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
