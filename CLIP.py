# ==========================
# üì¶ ÎùºÏù¥Î∏åÎü¨Î¶¨ Import
# ==========================

import torch
import torch.nn.functional as F
from transformers import (
    CLIPProcessor, CLIPModel,
    BlipProcessor, BlipForConditionalGeneration,
    ViltProcessor, ViltForQuestionAnswering
)
from PIL import Image
import json
import os
from tqdm import tqdm
from datasets import load_dataset

# ==========================
# ‚öôÔ∏è Î™®Îç∏ Î°úÎî©
# ==========================

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model.eval()

caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model.eval()

vqa_model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
vqa_processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
vqa_model.eval()

# ==========================
# üìÇ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï
# ==========================

COCO_IMAGE_DIR = "./val2017"
COCO_ANN_PATH = "annotations_trainval2017/annotations/captions_val2017.json"

# ==========================
# üìö Ï∫°ÏÖò Î°úÎî©
# ==========================

with open(COCO_ANN_PATH, 'r') as f:
    coco_data = json.load(f)

from collections import defaultdict
id_to_captions = defaultdict(list)
for ann in coco_data['annotations']:
    id_to_captions[ann['image_id']].append(ann['caption'])

id_to_filename = {img['id']: img['file_name'] for img in coco_data['images']}

# ==========================
# üîç Image-Text Retrieval (CLIP)
# ==========================

def encode_image_text(image, text):
    inputs = clip_processor(text=[text], images=image, return_tensors="pt", padding=True)
    outputs = clip_model(**inputs)
    return outputs.image_embeds, outputs.text_embeds

def cosine_similarity(img_embeds, text_embeds):
    img_embeds = F.normalize(img_embeds, dim=-1)
    text_embeds = F.normalize(text_embeds, dim=-1)
    return torch.matmul(img_embeds, text_embeds.T)

def compute_recall(similarity_matrix, k):
    correct = 0
    for i in range(similarity_matrix.size(0)):
        values, indices = similarity_matrix[i].topk(k)
        if i in indices:
            correct += 1
    return correct / similarity_matrix.size(0)

# ‚ñ∂ Ïú†Ìö®Ìïú Ïù¥ÎØ∏ÏßÄ IDÎßå ÌïÑÌÑ∞ÎßÅ
valid_ids = []
for img_id in id_to_captions.keys():
    if img_id in id_to_filename:
        file_path = os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id])
        if os.path.exists(file_path):
            valid_ids.append(img_id)

selected_ids = valid_ids[:100]
image_embeds_list = []
text_embeds_list = []

for img_id in tqdm(selected_ids, desc="Encoding images and captions"):
    file_path = os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id])
    image = Image.open(file_path).convert("RGB")
    caption = id_to_captions[img_id][0]
    img_embed, txt_embed = encode_image_text(image, caption)
    image_embeds_list.append(img_embed)
    text_embeds_list.append(txt_embed)

if len(image_embeds_list) == 0 or len(text_embeds_list) == 0:
    print("‚ùó No valid image-caption pairs found. Check val2017 image folder or selected IDs.")
    exit()

image_embeds_all = torch.cat(image_embeds_list, dim=0)
text_embeds_all = torch.cat(text_embeds_list, dim=0)

sim_matrix = cosine_similarity(image_embeds_all, text_embeds_all)
recall1 = compute_recall(sim_matrix, 1)
recall5 = compute_recall(sim_matrix, 5)
recall10 = compute_recall(sim_matrix, 10)

print(f"\n[Image-Text Retrieval Results - COCO]")
print(f"Recall@1: {recall1:.4f}")
print(f"Recall@5: {recall5:.4f}")
print(f"Recall@10: {recall10:.4f}")

# ==========================
# üìù Image Captioning (BLIP) + Ïù¥ÎØ∏ÏßÄ show()
# ==========================

def generate_caption(image):
    inputs = caption_processor(image, return_tensors="pt")
    out = caption_model.generate(**inputs)
    caption = caption_processor.decode(out[0], skip_special_tokens=True)
    return caption

print("\n[Image Captioning Results - COCO]")
for img_id in selected_ids[:5]:
    file_name = id_to_filename[img_id]
    file_path = os.path.join(COCO_IMAGE_DIR, file_name)
    image = Image.open(file_path).convert("RGB")
    gen_caption = generate_caption(image)
    print(f"Image: {file_name} | Generated Caption: {gen_caption}")
    image.show(title=f"{file_name}")

# ==========================
# ‚ùì Visual Question Answering (VQA)
# ==========================

vqa_data = load_dataset("vqa_v2", split="validation")

print("\n[VQA Results - COCO + HuggingFace vqa]")
for sample in vqa_data.select(range(5)):
    image_id = sample["image_id"]
    question = sample["question"]
    answer = sample["answer"]

    file_name = f"{image_id:012d}.jpg"
    file_path = os.path.join(COCO_IMAGE_DIR, file_name)
    if not os.path.exists(file_path):
        print(f"Image not found: {file_name}")
        continue

    image = Image.open(file_path).convert("RGB")
    encoding = vqa_processor(image, question, return_tensors="pt")
    output = vqa_model(**encoding)
    pred_id = output.logits.argmax(-1).item()
    pred_answer = vqa_model.config.id2label[pred_id]

    print(f"Q: {question}\nPredicted: {pred_answer} | Ground Truth: {answer}\n")