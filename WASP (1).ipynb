{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WASP 최종코드\n",
    "# 하단에 평가 코드(clip과의 비교)도 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정 및 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 인코더: BERT 기반 Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #랜덤 3개 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #평균 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS token만 추출 (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj\n",
    "        \n",
    "     # 텍스트만 인코딩\n",
    "    def encode_tokenized_input_text_only(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]\n",
    "            txt_proj = self.proj_text(txt_cls)\n",
    "            return txt_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss 계산 함수\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    \n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "json_path = \"coco_token_bbox_matched.json\"\n",
    "image_root = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "\n",
    "# 1. Load JSON once\n",
    "with open(json_path, \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# 2. 유효 이미지와 대표 label 추출\n",
    "valid_indices = []\n",
    "labels = []\n",
    "\n",
    "def get_dominant_label(matches):\n",
    "    if not matches:\n",
    "        return \"none\"\n",
    "    return Counter([m[\"label\"] for m in matches]).most_common(1)[0][0]\n",
    "\n",
    "for i, entry in enumerate(all_data):\n",
    "    image_id = entry[\"image_id\"]\n",
    "    image_path = os.path.join(image_root, f\"{image_id:012d}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        valid_indices.append(i)\n",
    "        labels.append(get_dominant_label(entry[\"matches\"]))\n",
    "\n",
    "# 3. stratified split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
    "subset_rel_idx, _ = next(splitter.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "# 4. 전체 JSON 기준으로 실제 subset 인덱스로 변환\n",
    "subset_json_indices = [valid_indices[i] for i in subset_rel_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10, subset_indices=None):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.data = []\n",
    "        selected = subset_indices if subset_indices is not None else range(len(all_data))\n",
    "\n",
    "        for i in selected:\n",
    "            entry = all_data[i]\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"유효 이미지 수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            print(f\"[WARN] 이미지 불러오기 실패: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            image_tensor = image\n",
    "\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "\n",
    "        # Tokenize caption\n",
    "        caption_tokens = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                        truncation=True, max_length=30)[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # Binary mask 생성\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        mask_tensor = torch.zeros((self.max_tokens, H, W))\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            mask_tensor[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "\n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'caption': caption,\n",
    "            'mask': mask_tensor,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch] \n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    return images, captions, masks, image_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효 이미지 수: 54966\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=json_path,\n",
    "    image_root=image_root,\n",
    "    transform=transform,\n",
    "    subset_indices=subset_json_indices\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=coco_collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #람다 점진적 높이기\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn, curr_attn: [B, T, P] - softmax된 attention map\n",
    "    \"\"\"\n",
    "    prev = torch.clamp(prev_attn, min=eps)\n",
    "    curr = torch.clamp(curr_attn, min=eps)\n",
    "    kl = (prev * (prev.log() - curr.log())).sum(dim=-1).mean()  # 평균 over T, B\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def train_warmup(model, dataloader, optimizer, device,\n",
    "                 lambda_cons=0.5, warmup_epochs=10, start_epoch=0):\n",
    "    model.train()\n",
    "    bbox_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.005, max_lambda=lambda_cons)\n",
    "\n",
    "    acc_history = []\n",
    "    prev_acc = None\n",
    "    drop_count = 0\n",
    "    high_point_epoch = None\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + warmup_epochs):\n",
    "        total_loss, total_acc = 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        curr_attn_dict = {}\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f\"Warm-up Epoch {epoch}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images, captions, masks, image_ids = batch\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_attn = attn_weights.size(1)\n",
    "            T_mask = masks.size(1)\n",
    "            T_common = min(T_attn, T_mask)\n",
    "\n",
    "            attn_weights_slice = attn_weights[:, :T_common, :]\n",
    "            masks_slice = masks[:, :T_common]\n",
    "\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            lambda_bbox, _ = bbox_scheduler.update(attn_weights_slice, return_diff=True)\n",
    "            loss_bbox_cons = compute_consistency_loss(attn_weights_slice, masks_slice)\n",
    "            loss_bbox_cons = torch.clamp(loss_bbox_cons, max=1.0)\n",
    "            loss = loss_contrastive + lambda_bbox * loss_bbox_cons\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                curr_attn_dict[str(img_id)] = attn_weights[i].detach().cpu()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "        acc_history.append(avg_acc)\n",
    "        print(f\"[WARM-UP] Epoch {epoch} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'attn_dict': curr_attn_dict},\n",
    "                   f\"temp_checkpoint_epoch{epoch}.pth\")\n",
    "\n",
    "        if prev_acc is not None and prev_acc > avg_acc:\n",
    "            drop_count += 1\n",
    "        else:\n",
    "            drop_count = 0\n",
    "\n",
    "        prev_acc = avg_acc\n",
    "\n",
    "        if drop_count >= 2:\n",
    "            high_point_epoch = epoch - 2\n",
    "            print(f\"[WARM-UP 종료 감지] Accuracy 하락 → 고점 epoch: {high_point_epoch}\")\n",
    "            shutil.copyfile(f\"temp_checkpoint_epoch{high_point_epoch}.pth\",\n",
    "                            f\"checkpoint_epoch{high_point_epoch}_stable.pth\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warm-up Epoch 0:   0%| | 1/13742 [00:00<2:18:37,  1.65it/s, loss=1.46, acc=0.250"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:   1%| | 129/13742 [00:14<28:35,  7.93it/s, loss=0.795, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:   9%| | 1172/13742 [02:04<21:41,  9.66it/s, loss=1.01, acc=0.75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  21%|▏| 2923/13742 [05:06<18:24,  9.79it/s, loss=0.636, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  30%|▎| 4165/13742 [07:14<16:37,  9.60it/s, loss=0.797, acc=0.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  39%|▍| 5396/13742 [09:22<14:27,  9.62it/s, loss=0.8, acc=0.750"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  42%|▍| 5773/13742 [10:01<13:37,  9.75it/s, loss=0.544, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  46%|▍| 6364/13742 [11:01<12:30,  9.84it/s, loss=0.502, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  60%|▌| 8264/13742 [14:18<09:24,  9.71it/s, loss=0.548, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  74%|▋| 10137/13742 [17:31<06:12,  9.68it/s, loss=0.64, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 0:  81%|▊| 11071/13742 [19:07<04:41,  9.49it/s, loss=0.544, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 0 - Avg Loss: 0.6769, Avg Accuracy: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:   7%| | 985/13742 [01:42<22:12,  9.58it/s, loss=0.528, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  11%| | 1497/13742 [02:34<21:18,  9.58it/s, loss=0.504, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  22%|▏| 3048/13742 [05:15<18:26,  9.67it/s, loss=0.836, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  40%|▍| 5441/13742 [09:21<14:07,  9.80it/s, loss=0.524, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  65%|▋| 8936/13742 [15:23<08:18,  9.64it/s, loss=0.802, acc=0.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  77%|▊| 10569/13742 [18:13<05:24,  9.77it/s, loss=0.501, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  79%|▊| 10828/13742 [18:40<05:03,  9.61it/s, loss=0.734, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  93%|▉| 12811/13742 [22:05<01:36,  9.67it/s, loss=0.537, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  96%|▉| 13257/13742 [22:51<00:50,  9.62it/s, loss=0.653, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 1:  99%|▉| 13548/13742 [23:21<00:19,  9.73it/s, loss=0.502, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 1 - Avg Loss: 0.6301, Avg Accuracy: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:   9%| | 1275/13742 [02:12<21:24,  9.70it/s, loss=0.519, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  20%|▏| 2680/13742 [04:37<18:49,  9.79it/s, loss=0.518, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  22%|▏| 3004/13742 [05:11<18:26,  9.70it/s, loss=0.523, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  31%|▎| 4253/13742 [07:19<16:15,  9.73it/s, loss=0.531, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  54%|▌| 7355/13742 [12:41<11:02,  9.64it/s, loss=0.5, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  55%|▌| 7603/13742 [13:06<10:29,  9.75it/s, loss=0.58, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  63%|▋| 8629/13742 [14:53<08:46,  9.71it/s, loss=0.771, acc=0.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  81%|▊| 11103/13742 [19:09<04:33,  9.66it/s, loss=0.603, acc=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  94%|▉| 12885/13742 [22:15<01:29,  9.60it/s, loss=1.04, acc=0.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2:  98%|▉| 13464/13742 [23:14<00:28,  9.82it/s, loss=0.524, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 2 - Avg Loss: 0.6374, Avg Accuracy: 0.9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:   8%| | 1152/13742 [02:00<22:18,  9.40it/s, loss=0.615, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  14%|▏| 1975/13742 [03:25<20:32,  9.55it/s, loss=0.556, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  22%|▏| 2986/13742 [05:10<18:17,  9.80it/s, loss=0.617, acc=0.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  43%|▍| 5877/13742 [10:09<13:30,  9.71it/s, loss=0.635, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  54%|▌| 7402/13742 [12:48<11:14,  9.40it/s, loss=0.528, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  54%|▌| 7456/13742 [12:53<11:07,  9.42it/s, loss=0.509, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  80%|▊| 11037/13742 [19:04<04:40,  9.65it/s, loss=0.542, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  83%|▊| 11406/13742 [19:42<04:04,  9.57it/s, loss=0.591, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  89%|▉| 12162/13742 [21:00<02:43,  9.67it/s, loss=0.928, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3:  91%|▉| 12460/13742 [21:31<02:12,  9.71it/s, loss=0.502, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARM-UP] Epoch 3 - Avg Loss: 0.6520, Avg Accuracy: 0.9457\n",
      "[WARM-UP 종료 감지] Accuracy 하락 → 고점 epoch: 1\n"
     ]
    }
   ],
   "source": [
    "# 1. 기기 설정 및 초기화\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 2. Warm-up 학습 함수 실행\n",
    "train_warmup(\n",
    "    model=model,\n",
    "    dataloader=dataloader,  # 사용 중인 dataloader 그대로 입력\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    lambda_cons=0.5,         # bbox consistency weight \n",
    "    warmup_epochs=10,       # 충분한 최대 epoch 설정\n",
    "    start_epoch=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_self_start.pth'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(\"checkpoint_epoch1_stable.pth\", \"checkpoint_self_start.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_self_supervised_start(path=\"checkpoint_self_start.pth\", lr=5e-5):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = VisionLanguageModel().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    prev_attn_dict = checkpoint['attn_dict']\n",
    "    return model, optimizer, prev_attn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_supervised(model, dataloader, optimizer, device,\n",
    "                          prev_attn_dict, lambda_self=0.1, num_epochs=10, start_epoch=0):\n",
    "    model.train()\n",
    "    acc_history = []\n",
    "    lambda_self_ㅏscheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=lambda_self)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc, num_batches = 0.0, 0.0, 0\n",
    "        curr_attn_dict = {}\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f\"Self Epoch {epoch}\", leave=False)\n",
    "        for batch_idx, batch in enumerate(progress):\n",
    "            images, captions, _, image_ids = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "            T_common = attn_weights.size(1)\n",
    "            attn_weights_slice = attn_weights[:, :T_common, :]\n",
    "\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            loss_self, count = 0.0, 0\n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                img_id_str = str(img_id)\n",
    "\n",
    "                if img_id_str in prev_attn_dict:\n",
    "                    prev_attn = prev_attn_dict[img_id_str].to(device)\n",
    "                    curr_attn = attn_weights[i]\n",
    "\n",
    "                    T_common = min(prev_attn.size(0), curr_attn.size(0))\n",
    "                    prev_attn_crop = prev_attn[:T_common, :]\n",
    "                    curr_attn_crop = curr_attn[:T_common, :]\n",
    "\n",
    "                    prev_soft = F.softmax(prev_attn_crop, dim=-1)\n",
    "                    curr_soft = F.softmax(curr_attn_crop, dim=-1)\n",
    "                    loss_self += kl_divergence_attention(prev_soft, curr_soft)\n",
    "\n",
    "                    diff = F.mse_loss(curr_attn_crop, prev_attn_crop, reduction='mean').item()\n",
    "                    alpha = max(0.1, min(0.9, 1.0 - diff))  # 차이 클수록 alpha 작게\n",
    "\n",
    "                    updated_attn = alpha * curr_attn_crop + (1 - alpha) * prev_attn_crop\n",
    "                    curr_attn_dict[img_id_str] = updated_attn.detach().cpu()\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            loss_self = loss_self / count if count > 0 else torch.tensor(0.0, device=device)\n",
    "            lambda_self_val, _ = lambda_self_scheduler.update(attn_weights_slice, return_diff=True)\n",
    "            loss = loss_contrastive + lambda_self_val * loss_self\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            num_batches += 1\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "        acc_history.append(avg_acc)\n",
    "\n",
    "        print(f\"[SELF] Epoch {epoch} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'attn_dict': curr_attn_dict\n",
    "        }, f\"temp_checkpoint_self_epoch{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Self Epoch 2:   0%|    | 2/13742 [00:00<27:49,  8.23it/s, loss=0.385, acc=0.750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:   2%| | 212/13742 [00:22<23:36,  9.55it/s, loss=0.0194, acc=1.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n",
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  20%|▏| 2700/13742 [04:43<19:29,  9.44it/s, loss=0.00392, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  21%|▍ | 2929/13742 [05:07<19:11,  9.39it/s, loss=0.34, acc=0.750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  22%|▏| 3041/13742 [05:19<18:45,  9.51it/s, loss=0.0704, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  26%|▎| 3613/13742 [06:19<17:33,  9.61it/s, loss=0.0157, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  31%|▎| 4313/13742 [07:32<16:15,  9.67it/s, loss=0.0014, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  61%|▌| 8427/13742 [14:43<09:12,  9.62it/s, loss=0.559, acc=0.500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  86%|▊| 11869/13742 [20:45<03:16,  9.55it/s, loss=0.000907, acc=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 2:  86%|▊| 11886/13742 [20:46<03:14,  9.56it/s, loss=0.0317, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 2 - Avg Loss: 0.1123, Avg Accuracy: 0.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:   9%| | 1206/13742 [02:07<21:57,  9.51it/s, loss=0.00921, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  32%|▎| 4383/13742 [07:41<16:40,  9.36it/s, loss=0.0332, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  33%|▎| 4480/13742 [07:51<16:30,  9.35it/s, loss=0.014, acc=1.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  35%|▎| 4812/13742 [08:27<15:52,  9.38it/s, loss=0.00064, acc=1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  39%|▍| 5357/13742 [09:25<15:07,  9.24it/s, loss=0.0107, acc=1.000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  59%|▌| 8061/13742 [14:10<10:04,  9.40it/s, loss=0.609, acc=0.500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  67%|▋| 9167/13742 [16:06<07:57,  9.59it/s, loss=0.000969, acc=1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  90%|▉| 12308/13742 [21:36<02:31,  9.44it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  92%|▉| 12628/13742 [22:10<01:58,  9.37it/s, loss=1.39, acc=0.500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 3:  93%|▉| 12728/13742 [22:21<01:48,  9.39it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 3 - Avg Loss: 0.3203, Avg Accuracy: 0.8391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  11%|▏ | 1550/13742 [02:44<21:33,  9.42it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  11%|▏ | 1575/13742 [02:47<21:23,  9.48it/s, loss=1.38, acc=0.500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  21%|▍ | 2906/13742 [05:08<19:04,  9.47it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  29%|▌ | 3974/13742 [07:02<17:12,  9.46it/s, loss=1.38, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  32%|▋ | 4351/13742 [07:42<16:36,  9.42it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  35%|▋ | 4823/13742 [08:31<15:45,  9.43it/s, loss=1.38, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  46%|▉ | 6337/13742 [11:12<12:59,  9.50it/s, loss=1.39, acc=0.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  76%|▊| 10378/13742 [18:16<05:47,  9.69it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  76%|▊| 10422/13742 [18:21<05:44,  9.63it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 4:  81%|▊| 11176/13742 [19:40<04:26,  9.63it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 4 - Avg Loss: 1.3864, Avg Accuracy: 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:   4%|   | 559/13742 [00:58<22:56,  9.58it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  19%|▍ | 2665/13742 [04:39<19:08,  9.64it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  34%|▋ | 4672/13742 [08:08<15:44,  9.61it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  41%|▊ | 5664/13742 [09:52<13:56,  9.65it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  71%|█▍| 9733/13742 [17:05<07:11,  9.29it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  83%|▊| 11454/13742 [20:09<04:05,  9.34it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  87%|▊| 12024/13742 [21:10<03:05,  9.25it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  88%|▉| 12101/13742 [21:19<02:55,  9.35it/s, loss=1.39, acc=0.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  88%|▉| 12156/13742 [21:25<02:49,  9.37it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 5:  97%|▉| 13394/13742 [23:37<00:38,  9.15it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 5 - Avg Loss: 1.3863, Avg Accuracy: 0.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:   1%|    | 82/13742 [00:08<24:08,  9.43it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000478812.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:   4%|   | 554/13742 [00:59<23:35,  9.32it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000453622.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  12%|▏ | 1669/13742 [02:57<21:50,  9.21it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000455533.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  43%|▊ | 5963/13742 [10:34<14:12,  9.13it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000017867.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  48%|▉ | 6541/13742 [11:36<13:01,  9.21it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000325021.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  79%|▊| 10831/13742 [19:18<05:09,  9.40it/s, loss=1.39, acc=0.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000322691.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  79%|▊| 10894/13742 [19:25<05:02,  9.41it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000329261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  85%|▊| 11630/13742 [20:44<03:43,  9.43it/s, loss=1.39, acc=0.250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000401218.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  87%|▊| 11932/13742 [21:16<03:13,  9.35it/s, loss=1.39, acc=0.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000429834.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Epoch 6:  92%|▉| 12652/13742 [22:34<01:56,  9.33it/s, loss=1.39, acc=0.500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 이미지 불러오기 실패: /raid/kyscap251/team2/train2017/train2017/000000389624.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SELF] Epoch 6 - Avg Loss: 1.3863, Avg Accuracy: 0.2509\n"
     ]
    }
   ],
   "source": [
    "#  고점 체크포인트 불러오기\n",
    "model, optimizer, prev_attn_dict = load_self_supervised_start()\n",
    "\n",
    "#  self-consistency 학습용 train_model 함수 실행\n",
    "train_self_supervised(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    lambda_self=0.1,\n",
    "    num_epochs=5, # 원하는 만큼 self-consistency 학습 횟수\n",
    "    start_epoch=2,\n",
    "    prev_attn_dict=prev_attn_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_checkpoint_self.pth'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(\"temp_checkpoint_self_epoch2.pth\", \"end_checkpoint_self.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가\n"
     ]
    }
   ],
   "source": [
    "print(\"평가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for entry in data:\n",
    "            img_id = entry['image_id']\n",
    "            captions = entry['captions']\n",
    "            if captions:\n",
    "                self.imgid2caption[img_id] = captions[0]  # 첫 번째 캡션만 사용\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "\n",
    "        try:\n",
    "            image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        except (FileNotFoundError, UnidentifiedImageError):\n",
    "            print(f\"[WARN] 이미지 불러오기 실패: {img_path}\")\n",
    "            # 다음 샘플로 넘어가기\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        caption = self.imgid2caption[img_id]\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"coco_token_bbox_matched.json\", \n",
    "    image_root=\"/shared/home/kyscap251/Team2/val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "eval_loader = DataLoader(eval_subset, batch_size=32, shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clip모델의 코사인 유사도 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking cosine similarity: 100%|███████████| 1000/1000 [01:36<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OpenCLIP] 전체 정답쌍 평균 cosine similarity: 0.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# OpenCLIP 모델 및 전처리\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name='ViT-B-32',\n",
    "    pretrained='laion2b_s34b_b79k'\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "# 데이터셋 준비\n",
    "caption_json_path = \"annotations/captions_val2017.json\"\n",
    "image_root = \"val2017\"\n",
    "\n",
    "with open(caption_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "###\n",
    "\n",
    "imgid2captions = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    imgid2captions[img_id].append(ann['caption'])\n",
    "\n",
    "# 나중에 사용할 때\n",
    "caption = imgid2captions[img_id][0]  # 첫 번째 캡션만 사용\n",
    "\n",
    "##3333\n",
    "\n",
    "# COCO 이미지 ID 정렬 및 존재 확인\n",
    "image_ids = sorted(list(imgid2caption.keys()))\n",
    "image_paths = {img_id: os.path.join(image_root, f\"{img_id:012d}.jpg\") for img_id in image_ids}\n",
    "valid_ids = [img_id for img_id in image_ids if os.path.exists(image_paths[img_id])]\n",
    "\n",
    "# 평가 개수 제한\n",
    "max_samples = 5000\n",
    "valid_ids = valid_ids[:max_samples]\n",
    "\n",
    "\n",
    "all_similarities = []  # 모든 정답쌍 유사도 저장\n",
    "\n",
    "for i, img_id in enumerate(tqdm(valid_ids, desc=\"Checking cosine similarity\")):\n",
    "    img_path = image_paths[img_id]\n",
    "    caption = imgid2caption[img_id]\n",
    "\n",
    "    try:\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    text = tokenizer([caption]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_feat = model.encode_image(image)\n",
    "        text_feat = model.encode_text(text)\n",
    "\n",
    "    image_feat = F.normalize(image_feat, dim=-1)\n",
    "    text_feat = F.normalize(text_feat, dim=-1)\n",
    "    sim = F.cosine_similarity(image_feat, text_feat, dim=-1).item()\n",
    "    all_similarities.append(sim)\n",
    "\n",
    "# 평균 유사도 출력\n",
    "avg_sim = sum(all_similarities) / len(all_similarities)\n",
    "print(f\"\\n[OpenCLIP] 전체 정답쌍 평균 cosine similarity: {avg_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WASP의 정답 쌍 코사인 유사도 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Checking cosine similarity (VLM): 100%|███████| 500/500 [00:48<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VisionLanguageModel] 전체 정답쌍 평균 cosine similarity: 0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os, json\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 모델 및 환경 설정\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = VisionLanguageModel().to(device)\n",
    "ckpt = torch.load(\"end_checkpoint_self.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# tokenizer 및 transform 정의\n",
    "# ----------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 데이터 로딩\n",
    "# ----------------------------\n",
    "COCO_IMAGE_DIR = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "COCO_ANN_PATH = \"val_coco_token_bbox_matched.json\"\n",
    "\n",
    "with open(COCO_ANN_PATH, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "id_to_captions = defaultdict(list)\n",
    "id_to_filename = {}\n",
    "for ann in coco_data:\n",
    "    img_id = ann['image_id']\n",
    "    if ann['captions']:\n",
    "        id_to_captions[img_id].extend(ann['captions'])\n",
    "        id_to_filename[img_id] = f\"{img_id:012d}.jpg\"\n",
    "\n",
    "# ----------------------------\n",
    "# 유효 이미지 추리기\n",
    "# ----------------------------\n",
    "valid_ids = [img_id for img_id in id_to_filename if os.path.exists(os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id]))]\n",
    "selected_ids = valid_ids[:5000]  # 평가 개수 제한\n",
    "\n",
    "# ----------------------------\n",
    "# 정답쌍 유사도 측정\n",
    "# ----------------------------\n",
    "all_similarities = []\n",
    "\n",
    "for i, img_id in enumerate(tqdm(selected_ids, desc=\"Checking cosine similarity (VLM)\")):\n",
    "    img_path = os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id])\n",
    "    caption = id_to_captions[img_id][0]  # 첫 번째 정답 캡션만 사용\n",
    "\n",
    "    try:\n",
    "        image = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] 이미지 오류: {img_path} | {e}\")\n",
    "        continue\n",
    "\n",
    "    tokens = tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_feat, txt_feat = model.encode_tokenized_input(image, input_ids, attention_mask)\n",
    "\n",
    "    img_feat = F.normalize(img_feat, dim=-1)\n",
    "    txt_feat = F.normalize(txt_feat, dim=-1)\n",
    "    sim = F.cosine_similarity(img_feat, txt_feat, dim=-1).item()\n",
    "    all_similarities.append(sim)\n",
    "\n",
    "# ----------------------------\n",
    "# 평균 유사도 출력\n",
    "# ----------------------------\n",
    "avg_sim = sum(all_similarities) / len(all_similarities)\n",
    "print(f\"\\n[VisionLanguageModel] 전체 정답쌍 평균 cosine similarity: {avg_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WASP의 cosine smiliarity 오답 쌍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VisionLanguageModel] 정답이 아닌 쌍 평균 cosine similarity (5000개 배치 처리): 0.0958\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "negative_similarities = []\n",
    "\n",
    "for i in range(0, len(selected_ids), batch_size):\n",
    "    batch_ids = selected_ids[i:i+batch_size]\n",
    "    images = []\n",
    "    neg_captions = []\n",
    "\n",
    "    for img_id in batch_ids:\n",
    "        img_path = os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id])\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        try:\n",
    "            image = transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        except:\n",
    "            continue\n",
    "        images.append(image)\n",
    "\n",
    "        # 랜덤한 다른 이미지의 캡션\n",
    "        while True:\n",
    "            neg_id = random.choice(selected_ids)\n",
    "            if neg_id != img_id and id_to_captions[neg_id]:\n",
    "                break\n",
    "        neg_captions.append(id_to_captions[neg_id][0])\n",
    "\n",
    "    if not images or not neg_captions:\n",
    "        continue\n",
    "\n",
    "    images = torch.stack(images).to(device)\n",
    "    tokens = tokenizer(neg_captions, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_feats, txt_feats = model.encode_tokenized_input(images, input_ids, attention_mask)\n",
    "\n",
    "    img_feats = F.normalize(img_feats, dim=-1)\n",
    "    txt_feats = F.normalize(txt_feats, dim=-1)\n",
    "    sims = F.cosine_similarity(img_feats, txt_feats, dim=-1).tolist()\n",
    "    negative_similarities.extend(sims)\n",
    "\n",
    "avg_neg_sim = sum(negative_similarities) / len(negative_similarities)\n",
    "print(f\"\\n[VisionLanguageModel] 정답이 아닌 쌍 평균 cosine similarity (5000개 배치 처리): {avg_neg_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clip의 image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CocoClipEvaluationDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            self.imgid2caption[img_id] = ann['caption']\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        caption = self.imgid2caption[img_id]\n",
    "        return image, caption  # 2개 값만 반환\n",
    "\n",
    "\n",
    "\n",
    "def openclip_retrieval_evaluate(model, val_loader, tokenizer, device, max_samples=5000):\n",
    "    model.eval()\n",
    "    image_embeds_list = []\n",
    "    text_embeds_list = []\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 2개 값만 언패킹 (images, texts)\n",
    "        for images, texts in tqdm(val_loader, desc=\"Encoding image-text pairs\"):\n",
    "            images = images.to(device)\n",
    "            # 텍스트 토크나이징\n",
    "            text_tokens = tokenizer(texts).to(device)\n",
    "            \n",
    "            # 임베딩 추출\n",
    "            image_embeds = model.encode_image(images)\n",
    "            text_embeds = model.encode_text(text_tokens)\n",
    "            \n",
    "            image_embeds_list.append(image_embeds)\n",
    "            text_embeds_list.append(text_embeds)\n",
    "            n_samples += images.size(0)\n",
    "            if n_samples >= max_samples:\n",
    "                break\n",
    "\n",
    "    image_embeds_all = torch.cat(image_embeds_list, dim=0)[:max_samples]\n",
    "    text_embeds_all = torch.cat(text_embeds_list, dim=0)[:max_samples]\n",
    "\n",
    "    image_embeds_all = F.normalize(image_embeds_all, dim=-1)\n",
    "    text_embeds_all = F.normalize(text_embeds_all, dim=-1)\n",
    "\n",
    "    sim_matrix = torch.matmul(image_embeds_all, text_embeds_all.T)\n",
    "\n",
    "    def compute_recall(sim_matrix, k):\n",
    "        correct = 0\n",
    "        for i in range(sim_matrix.size(0)):\n",
    "            topk = sim_matrix[i].topk(k).indices\n",
    "            if i in topk:\n",
    "                correct += 1\n",
    "        return correct / sim_matrix.size(0)\n",
    "\n",
    "    recall1 = compute_recall(sim_matrix, 1)\n",
    "    recall5 = compute_recall(sim_matrix, 5)\n",
    "    recall10 = compute_recall(sim_matrix, 10)\n",
    "\n",
    "    print(\"\\n[Image-Text Retrieval Results (Closed-domain)]\")\n",
    "    print(f\"Recall@1: {recall1:.4f}\")\n",
    "    print(f\"Recall@5: {recall5:.4f}\")\n",
    "    print(f\"Recall@10: {recall10:.4f}\")\n",
    "\n",
    "    return recall1, recall5, recall10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 전용 transform\n",
    "_, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "clip_val_dataset = CocoClipEvaluationDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=preprocess_clip\n",
    ")\n",
    "\n",
    "clip_val_loader = DataLoader(\n",
    "    clip_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding image-text pairs:  99%|█████████████▉| 156/157 [08:08<00:03,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Image-Text Retrieval Results (Closed-domain)]\n",
      "Recall@1: 0.4108\n",
      "Recall@5: 0.6770\n",
      "Recall@10: 0.7716\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 초기화\n",
    "model_clip, _, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer_clip = open_clip.get_tokenizer('ViT-B-32')\n",
    "device_clip = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_clip = model_clip.to(device_clip)\n",
    "\n",
    "# 평가 실행\n",
    "recall1, recall5, recall10 = openclip_retrieval_evaluate(\n",
    "    model_clip,\n",
    "    clip_val_loader,\n",
    "    tokenizer_clip,\n",
    "    device_clip,\n",
    "    max_samples=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 내 모델의 image retrival 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Encoding: 100%|█████████████████████████████████| 16/16 [00:48<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Image-Text Retrieval Results (Your Model)]\n",
      "Recall@1: 0.0980\n",
      "Recall@5: 0.2800\n",
      "Recall@10: 0.4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "import os, json, random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# 모델 및 환경 설정\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionLanguageModel().to(device)\n",
    "ckpt = torch.load(\"end_checkpoint_self.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# tokenizer 및 transform 정의\n",
    "# ----------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 데이터셋 정의\n",
    "# ----------------------------\n",
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, image_dir, ann_path, transform, tokenizer, max_samples=500):\n",
    "        with open(ann_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        self.imgid2filename = {}\n",
    "        for entry in data:\n",
    "            img_id = entry['image_id']\n",
    "            captions = entry['captions']\n",
    "            if captions:\n",
    "                self.imgid2caption[img_id] = captions[0]\n",
    "                self.imgid2filename[img_id] = f\"{img_id:012d}.jpg\"\n",
    "\n",
    "        valid_ids = [img_id for img_id in self.imgid2filename if os.path.exists(os.path.join(image_dir, self.imgid2filename[img_id]))]\n",
    "        self.selected_ids = valid_ids[:max_samples]\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.selected_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.selected_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, self.imgid2filename[img_id])\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        caption = self.imgid2caption[img_id]\n",
    "        tokens = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n",
    "\n",
    "# ----------------------------\n",
    "# 평가 함수 정의\n",
    "# ----------------------------\n",
    "def evaluate_recall(model, dataloader, device):\n",
    "    image_feats, text_feats = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            img_feat, txt_feat = model.encode_tokenized_input(images, input_ids, attention_mask)\n",
    "            image_feats.append(img_feat)\n",
    "            text_feats.append(txt_feat)\n",
    "\n",
    "    image_feats = F.normalize(torch.cat(image_feats, dim=0), dim=-1)\n",
    "    text_feats = F.normalize(torch.cat(text_feats, dim=0), dim=-1)\n",
    "    sim_matrix = torch.matmul(image_feats, text_feats.T)\n",
    "\n",
    "    def compute_recall(sim_matrix, k):\n",
    "        correct = 0\n",
    "        for i in range(sim_matrix.size(0)):\n",
    "            topk = sim_matrix[i].topk(k).indices\n",
    "            if i in topk:\n",
    "                correct += 1\n",
    "        return correct / sim_matrix.size(0)\n",
    "\n",
    "    recall1 = compute_recall(sim_matrix, 1)\n",
    "    recall5 = compute_recall(sim_matrix, 5)\n",
    "    recall10 = compute_recall(sim_matrix, 10)\n",
    "\n",
    "    print(\"\\n[Image-Text Retrieval Results (Your Model)]\")\n",
    "    print(f\"Recall@1: {recall1:.4f}\")\n",
    "    print(f\"Recall@5: {recall5:.4f}\")\n",
    "    print(f\"Recall@10: {recall10:.4f}\")\n",
    "\n",
    "    return recall1, recall5, recall10\n",
    "\n",
    "# ----------------------------\n",
    "# 실행\n",
    "# ----------------------------\n",
    "dataset = CustomCocoDataset(\n",
    "    image_dir=\"/shared/home/kyscap251/Team2/val2017\",\n",
    "    ann_path=\"val_coco_token_bbox_matched.json\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer,\n",
    "    max_samples=5000\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "recall1, recall5, recall10 = evaluate_recall(model, loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### image captioning open clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions: 100%|████████████████████| 500/500 [00:46<00:00, 10.78it/s]\n",
      "PTBTokenizer tokenized 6175 tokens at 28956.54 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Indirect Captioning Evaluation - CIDEr Metric (Open-domain)]\n",
      "CIDEr: 1.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 6187 tokens at 63048.88 tokens per second.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os, json, random\n",
    "from tqdm import tqdm\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 환경 설정\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CAPTION_JSON_PATH = \"annotations/captions_val2017.json\"\n",
    "IMAGE_DIR = \"val2017\"\n",
    "max_images = 500  # 평가할 이미지 수\n",
    "caption_pool_size = 5000  # caption pool 크기\n",
    "\n",
    "# ----------------------------\n",
    "# OpenCLIP 모델 및 전처리\n",
    "# ----------------------------\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "# ----------------------------\n",
    "# 캡션 및 이미지 정보 로딩\n",
    "# ----------------------------\n",
    "with open(CAPTION_JSON_PATH, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "imgid2caption = {}\n",
    "caption_pool = []\n",
    "\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    if img_id not in imgid2caption:\n",
    "        imgid2caption[img_id] = caption\n",
    "    caption_pool.append(caption)\n",
    "\n",
    "# caption pool 샘플링\n",
    "caption_pool = list(set(caption_pool))\n",
    "random.shuffle(caption_pool)\n",
    "caption_pool = caption_pool[:caption_pool_size]\n",
    "\n",
    "# ----------------------------\n",
    "# caption pool 임베딩\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    tokenized_caps = tokenizer(caption_pool).to(device)\n",
    "    caption_feats = model.encode_text(tokenized_caps)\n",
    "    caption_feats = F.normalize(caption_feats, dim=-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 이미지 인코딩 및 가장 유사한 캡션 추출\n",
    "# ----------------------------\n",
    "image_ids = list(imgid2caption.keys())\n",
    "random.shuffle(image_ids)\n",
    "image_ids = image_ids[:max_images]\n",
    "\n",
    "generated = {}  # {img_id: [retrieved_caption]}\n",
    "references = {}  # {img_id: [gt_caption]}\n",
    "\n",
    "for img_id in tqdm(image_ids, desc=\"Generating captions\"):\n",
    "    img_path = os.path.join(IMAGE_DIR, f\"{img_id:012d}.jpg\")\n",
    "    try:\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_feat = model.encode_image(image)\n",
    "        image_feat = F.normalize(image_feat, dim=-1)\n",
    "\n",
    "    sims = torch.matmul(image_feat, caption_feats.T).squeeze(0)  # (pool_size,)\n",
    "    best_idx = sims.argmax().item()\n",
    "    retrieved_caption = caption_pool[best_idx]\n",
    "\n",
    "    generated[str(img_id)] = [retrieved_caption]\n",
    "    references[str(img_id)] = [imgid2caption[img_id]]\n",
    "\n",
    "# ----------------------------\n",
    "# CIDEr 점수 계산\n",
    "# ----------------------------\n",
    "def wrap_captions(d: dict) -> dict:\n",
    "    return {\n",
    "        str(k): [{\"caption\": c} for c in v] for k, v in d.items()\n",
    "    }\n",
    "\n",
    "# Tokenizer용 포맷으로 변환\n",
    "gts = wrap_captions(references)\n",
    "res = wrap_captions(generated)\n",
    "\n",
    "# 평가\n",
    "tokenizer = PTBTokenizer()\n",
    "gts_tok = tokenizer.tokenize(gts)\n",
    "res_tok = tokenizer.tokenize(res)\n",
    "\n",
    "cider_scorer = Cider()\n",
    "score, _ = cider_scorer.compute_score(gts_tok, res_tok)\n",
    "\n",
    "print(f\"\\n[Indirect Captioning Evaluation - CIDEr Metric (Open-domain)]\")\n",
    "print(f\"CIDEr: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WASP image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generating captions: 100%|████████████████████| 500/500 [00:50<00:00,  9.86it/s]\n",
      "PTBTokenizer tokenized 6137 tokens at 69041.10 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Indirect Captioning Evaluation - CIDEr Metric (Open-domain)]\n",
      "CIDEr: 0.4171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 6096 tokens at 57335.21 tokens per second.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import os, json, random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# 환경 설정\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "COCO_IMAGE_DIR = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "COCO_ANN_PATH = \"val_coco_token_bbox_matched.json\"\n",
    "max_images = 500\n",
    "caption_pool_size = 5000\n",
    "\n",
    "# ----------------------------\n",
    "# 모델 로딩\n",
    "# ----------------------------\n",
    "model = VisionLanguageModel().to(device)\n",
    "ckpt = torch.load(\"end_checkpoint_self.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# tokenizer 및 transform 정의\n",
    "# ----------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# COCO 캡션 데이터 로딩\n",
    "# ----------------------------\n",
    "with open(COCO_ANN_PATH, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "id_to_captions = defaultdict(list)\n",
    "id_to_filename = {}\n",
    "\n",
    "for ann in coco_data:\n",
    "    img_id = ann['image_id']\n",
    "    if ann['captions']:\n",
    "        id_to_captions[img_id].extend(ann['captions'])\n",
    "        id_to_filename[img_id] = f\"{img_id:012d}.jpg\"\n",
    "\n",
    "# caption pool 구성\n",
    "caption_pool = list(set([cap for caps in id_to_captions.values() for cap in caps]))\n",
    "random.shuffle(caption_pool)\n",
    "caption_pool = caption_pool[:caption_pool_size]\n",
    "\n",
    "# caption pool 임베딩\n",
    "with torch.no_grad():\n",
    "    all_caption_feats = []\n",
    "    for i in range(0, len(caption_pool), 32):\n",
    "        caps = caption_pool[i:i+32]\n",
    "        tokenized = tokenizer(caps, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = tokenized[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "        text_feat = model.encode_tokenized_input_text_only(input_ids, attention_mask)\n",
    "\n",
    "        all_caption_feats.append(text_feat)\n",
    "\n",
    "    caption_feats = torch.cat(all_caption_feats, dim=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 이미지 인코딩 + caption 매칭\n",
    "# ----------------------------\n",
    "valid_ids = [img_id for img_id in id_to_filename if os.path.exists(os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id]))]\n",
    "random.shuffle(valid_ids)\n",
    "selected_ids = valid_ids[:max_images]\n",
    "\n",
    "generated = {}   # {img_id: [retrieved_caption]}\n",
    "references = {}  # {img_id: [gt_caption]}\n",
    "\n",
    "for img_id in tqdm(selected_ids, desc=\"Generating captions\"):\n",
    "    img_path = os.path.join(COCO_IMAGE_DIR, id_to_filename[img_id])\n",
    "    try:\n",
    "        image = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    tokens = tokenizer(id_to_captions[img_id][0], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_feat, _ = model.encode_tokenized_input(image, input_ids, attention_mask)\n",
    "        img_feat = F.normalize(img_feat, dim=-1)\n",
    "\n",
    "    sims = torch.matmul(img_feat, caption_feats.T).squeeze(0)\n",
    "    best_idx = sims.argmax().item()\n",
    "    retrieved_caption = caption_pool[best_idx]\n",
    "\n",
    "    generated[str(img_id)] = [retrieved_caption]\n",
    "    references[str(img_id)] = [id_to_captions[img_id][0]]\n",
    "\n",
    "# ----------------------------\n",
    "# CIDEr 계산\n",
    "# ----------------------------\n",
    "def wrap_captions(d):\n",
    "    return {str(k): [{\"caption\": c} for c in v] for k, v in d.items()}\n",
    "\n",
    "gts = wrap_captions(references)\n",
    "res = wrap_captions(generated)\n",
    "\n",
    "ptb_tokenizer = PTBTokenizer()\n",
    "gts_tok = ptb_tokenizer.tokenize(gts)\n",
    "res_tok = ptb_tokenizer.tokenize(res)\n",
    "\n",
    "cider_scorer = Cider()\n",
    "score, _ = cider_scorer.compute_score(gts_tok, res_tok)\n",
    "\n",
    "print(f\"\\n[Indirect Captioning Evaluation - CIDEr Metric (Open-domain)]\")\n",
    "print(f\"CIDEr: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu118env)",
   "language": "python",
   "name": "cu118env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
