{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val ['000000433103.jpg', '000000129113.jpg', '000000196843.jpg', '000000252507.jpg', '000000258541.jpg']\n",
      "train ['000000254879.jpg', '000000316649.jpg', '000000430989.jpg', '000000286349.jpg', '000000458365.jpg']\n",
      "val파일 개수: 5000\n",
      "train파일 개수: 109932\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "val = \"val2017\"\n",
    "train_path = \"/raid/kyscap251/team2/train2017/train2017\"\n",
    "val_path = \"/shared/home/kyscap251/Team2/val2017\"\n",
    "# folder_path = \"/raid/kyscap251/team2/val2017/val2017\"\n",
    "test = \"train2017\"\n",
    "\n",
    "val_items = os.listdir(val_path)\n",
    "train_items = os.listdir(train_path)\n",
    "\n",
    "print(\"val\", val_items[:5])\n",
    "print(\"train\", train_items[:5])\n",
    "\n",
    "# 파일만 필터링\n",
    "files = [f for f in os.listdir(val_path)\n",
    "         if os.path.isfile(os.path.join(val_path, f))]\n",
    "\n",
    "print(f\"val파일 개수: {len(files)}\")\n",
    "\n",
    "filess = [ff for ff in os.listdir(train_path)\n",
    "         if os.path.isfile(os.path.join(train_path, ff))]\n",
    "print(f\"train파일 개수: {len(filess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name: NVIDIA A40\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 13 16:32:31 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n",
      "No device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/kyscap251/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정 및 라이브러리 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이미지 인코더: ViT\n",
    "# class VisionEncoder(nn.Module):\n",
    "#     def __init__(self, output_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.vit = models.vit_b_16(pretrained=True)\n",
    "#         self.vit.heads = nn.Identity()  # classification head 제거\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         patch_feats = self.vit(images)  # [B, D]\n",
    "#         return patch_feats\n",
    "    \n",
    "\n",
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # [B, 1+P, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 인코더: BERT 기반 Transformer\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, captions):\n",
    "        tokenized = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(self.bert.device)\n",
    "        outputs = self.bert(**tokenized)\n",
    "        return outputs.last_hidden_state, tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, text_emb, image_patches):\n",
    "        attn_output, attn_weights = self.attn(text_emb, image_patches, image_patches)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델\n",
    "# class VisionLanguageModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.vision_encoder = VisionEncoder()\n",
    "#         self.text_encoder = TextEncoder()\n",
    "#         self.cross_attn = CrossAttentionBlock()\n",
    "#         self.proj_image = nn.Linear(768, 512)\n",
    "#         self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "#     def forward(self, images, captions):\n",
    "#         img_feat = self.vision_encoder(images)\n",
    "#         text_emb, tokens = self.text_encoder(captions)\n",
    "#         patch_feat = img_feat.unsqueeze(1)  # dummy patch feature (B, 1, D)\n",
    "#         cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)\n",
    "#         img_proj = self.proj_image(img_feat)\n",
    "#         text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "#     def encode_for_inference(self, images, captions):\n",
    "#         with torch.no_grad():\n",
    "#             img_feat = self.vision_encoder(images)\n",
    "#             text_emb, _ = self.text_encoder(captions)\n",
    "#             img_proj = self.proj_image(img_feat)\n",
    "#             text_proj = self.proj_text(text_emb[:, 0])  # CLS token 기준\n",
    "#         return img_proj, text_proj\n",
    "    \n",
    "# -------------------------------\n",
    "# Vision-Language Model\n",
    "# -------------------------------\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.cross_attn = CrossAttentionBlock()\n",
    "        self.proj_image = nn.Linear(768, 512)\n",
    "        self.proj_text = nn.Linear(768, 512)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_feat_all = self.vision_encoder(images)  # [B, 1+P, D]\n",
    "        cls_feat = img_feat_all[:, 0]               # [B, D] CLS\n",
    "        patch_feat = img_feat_all[:, 1:]            # [B, P, D] patches only\n",
    "\n",
    "        text_emb, tokens = self.text_encoder(captions)  # [B, T, D]\n",
    "        cross_out, attn_weights = self.cross_attn(text_emb, patch_feat)  # [B, T, D], [B, T, P]\n",
    "\n",
    "        img_proj = self.proj_image(cls_feat)        # [B, 512]\n",
    "        text_proj = self.proj_text(text_emb[:, 0])  # [B, 512]\n",
    "\n",
    "        return img_proj, text_proj, attn_weights, tokens\n",
    "\n",
    "    def encode_for_inference(self, images, captions): #랜덤 3개 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)\n",
    "            cls_feat = img_feat_all[:, 0]\n",
    "            text_emb, _ = self.text_encoder(captions)\n",
    "            img_proj = self.proj_image(cls_feat)\n",
    "            text_proj = self.proj_text(text_emb[:, 0])\n",
    "        return img_proj, text_proj\n",
    "    \n",
    "    def encode_tokenized_input(self, images, input_ids, attention_mask): #평균 유사도 평가용\n",
    "        with torch.no_grad():\n",
    "            img_feat_all = self.vision_encoder(images)        # (B, 197, 768)\n",
    "            cls_feat = img_feat_all[:, 0, :]                  # CLS token만 추출 (B, 768)\n",
    "            img_proj = self.proj_image(cls_feat)              # (B, 512)\n",
    "\n",
    "            bert_out = self.text_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            txt_cls = bert_out.last_hidden_state[:, 0, :]     # (B, 768)\n",
    "            txt_proj = self.proj_text(txt_cls)                # (B, 512)\n",
    "\n",
    "            return img_proj, txt_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Loss 계산 함수\n",
    "def compute_consistency_loss(attn_weights, masks, eps=1e-6):\n",
    "    B, T, H, W = masks.shape\n",
    "    masks_flat = masks.view(B, T, -1)\n",
    "    scores = (attn_weights * masks_flat).sum(dim=-1)\n",
    "    scores = torch.clamp(scores, min=eps, max=1.0)\n",
    "    return -torch.log(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "\n",
    "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    logits = image_embeds @ text_embeds.T / temperature\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 JSON 로딩 및 binary mask 생성\n",
    "class CocoVLMDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, transform=None, patch_size=16, max_tokens=10):\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.data = []\n",
    "        for entry in all_data:\n",
    "            image_id = entry[\"image_id\"]\n",
    "            image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                self.data.append(entry)\n",
    "\n",
    "        print(f\"유효 이미지 수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        image_id = entry[\"image_id\"]\n",
    "        image_path = os.path.join(self.image_root, f\"{image_id:012d}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            print(f\"[WARN] 이미지 불러오기 실패: {image_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # 다음 인덱스로 재시도\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        captions = entry[\"captions\"]\n",
    "        matches = entry[\"matches\"][:self.max_tokens]\n",
    "        caption = captions[0]\n",
    "        H, W = 224 // self.patch_size, 224 // self.patch_size\n",
    "        masks = torch.zeros((self.max_tokens, H, W))\n",
    "        for i, match in enumerate(matches):\n",
    "            x, y, w, h = match[\"bbox\"]\n",
    "            x1 = int(x // self.patch_size)\n",
    "            y1 = int(y // self.patch_size)\n",
    "            x2 = int((x + w) // self.patch_size)\n",
    "            y2 = int((y + h) // self.patch_size)\n",
    "            masks[i, y1:y2+1, x1:x2+1] = 1.0\n",
    "        return image, caption, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    captions = [item[1] for item in batch]\n",
    "    masks = torch.stack([item[2] for item in batch])\n",
    "    return images, captions, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효 이미지 수: 5000\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CocoVLMDataset(\n",
    "    json_path=\"val_coco_token_bbox_matched.json\",\n",
    "#     image_root=\"/raid/kyscap251/team2/train2017/train2017\",\n",
    "    image_root = \"/shared/home/kyscap251/Team2/val2017\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=coco_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaScheduler: #람다 점진적 높이기\n",
    "    def __init__(self, alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5):\n",
    "        self.ema = None\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.max_lambda = max_lambda\n",
    "        self.apply = False\n",
    "        self.step = 0\n",
    "        self.current_lambda = 0.0\n",
    "\n",
    "    def update(self, attn_weights, return_diff=False):\n",
    "        \"\"\"\n",
    "        attn_weights: Tensor of shape [B, T, P] (cross-attn weights)\n",
    "        \"\"\"\n",
    "        attn_mean = attn_weights.mean(dim=1).mean(dim=0)  # [P]\n",
    "        if self.ema is None:\n",
    "            self.ema = attn_mean\n",
    "        else:\n",
    "            self.ema = self.alpha * attn_mean + (1 - self.alpha) * self.ema\n",
    "\n",
    "        diff = torch.abs(self.ema - attn_mean).mean().item()\n",
    "\n",
    "        if not self.apply and diff < self.threshold:\n",
    "            self.apply = True\n",
    "            print(f\"[LambdaScheduler] Consistency loss ON (diff={diff:.6f})\")\n",
    "\n",
    "        if self.apply:\n",
    "            self.current_lambda = min(self.max_lambda, self.anneal_rate * self.step)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.current_lambda = 0.0\n",
    "\n",
    "        if return_diff:\n",
    "            return self.current_lambda, diff\n",
    "        else:\n",
    "            return self.current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Epoch 학습 루프 (Matched Token만 사용하도록 수정)\n",
    "def train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=5):\n",
    "    model.train()\n",
    "#     ////////////\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "#     ////////////////////\n",
    "#     lambda_log = [] #λ 변화 기록용 리스트\n",
    "#     diff_log = [] #diff값 기록용 리스트\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            # matched token 수만큼 attention weight 슬라이싱\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :] #bbox에 해당하는 token만 consistency loss 계산에 사용\n",
    "\n",
    "            \n",
    "#             //////////////\n",
    "            # 현재 λ 값을 EMA 기반으로 계산 및 기록\n",
    "            lambda_cons, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "#             lambda_log.append(lambda_cons)\n",
    "#             diff_log.append(diff)\n",
    "# /////////////////////////////////////\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "            loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "            loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # cosine similarity + accuracy 계산\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                sims = torch.diag(sim_matrix)\n",
    "                sim_mean = sims.mean().item()\n",
    "                sim_std = sims.std().item()\n",
    "\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"cos_sim\": f\"{sim_mean:.3f}±{sim_std:.3f}\", \"acc\": f\"{acc:.3f}\"})\n",
    "\n",
    "#         # scheduler 업데이트\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "#             print(f\"[Epoch {epoch+1}] LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4248c3f7bd4517bfa9c8e008b28be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d1bfc43cb46968e6b8427c9d42bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Avg Loss: 0.9415, Avg Accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Avg Loss: 0.7531, Avg Accuracy: 0.9510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Avg Loss: 0.7454, Avg Accuracy: 0.9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# # 모델 학습 실행\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VisionLanguageModel().to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# train_model(model, dataloader, optimizer, device, lambda_cons=0.05, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 3개 추론\n",
    "def run_batch_inference(model, val_image_dir, caption_json_path, transform, device, sample_size=3):\n",
    "    with open(caption_json_path, 'r') as f:\n",
    "        coco_captions = json.load(f)\n",
    "\n",
    "    # image_id → caption 매핑\n",
    "    imgid2caption = {}\n",
    "    for ann in coco_captions['annotations']:\n",
    "        imgid = ann['image_id']\n",
    "        if imgid not in imgid2caption:\n",
    "            imgid2caption[imgid] = []\n",
    "        imgid2caption[imgid].append(ann['caption'])\n",
    "\n",
    "    # 랜덤 샘플링 (image_id 3개)\n",
    "    img_ids = random.sample(list(imgid2caption.keys()), sample_size)\n",
    "    captions = [imgid2caption[i][0] for i in img_ids]\n",
    "    image_paths = [os.path.join(val_image_dir, f\"{i:012d}.jpg\") for i in img_ids]\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.open(p).convert(\"RGB\")) for p in image_paths\n",
    "    ]).to(device)\n",
    "\n",
    "    # 인코딩\n",
    "    model.eval()\n",
    "    image_embeds, text_embeds = model.encode_for_inference(images_tensor, captions)\n",
    "\n",
    "    # 코사인 유사도\n",
    "    sim_matrix = F.cosine_similarity(image_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=-1)\n",
    "\n",
    "    # 출력\n",
    "    print(\"image_embeds shape:\", image_embeds.shape)\n",
    "    print(\"text_embeds shape :\", text_embeds.shape)\n",
    "    print(\"\\n\\U0001F4CA Cosine Similarity Matrix:\\n\")\n",
    "    for i, img_id in enumerate(img_ids):\n",
    "        print(f\"\\U0001F5BC️ {img_id:012d}.jpg\")\n",
    "        for j, cap in enumerate(captions):\n",
    "            print(f\"  \\\"{cap}\\\" → similarity: {sim_matrix[i, j]:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_inference(\n",
    "    model,\n",
    "    val_image_dir=\"val2017\",\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    sample_size=3 #랜덤으로 이미지 세장\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleCocoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_json_path, image_root, transform, tokenizer):\n",
    "        with open(caption_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.imgid2caption = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.imgid2caption:\n",
    "                self.imgid2caption[img_id] = ann['caption']\n",
    "\n",
    "        self.image_ids = list(self.imgid2caption.keys())\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_root, f\"{img_id:012d}.jpg\")\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        caption = self.imgid2caption[img_id]\n",
    "\n",
    "        encoding = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "val_dataset = SimpleCocoCaptionDataset(\n",
    "    caption_json_path=\"annotations/captions_val2017.json\",\n",
    "    image_root=\"val2017\",\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답,오답쌍 평균 유사도 계산 추론\n",
    "def evaluate_mean_similarity(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct_sim = 0.0\n",
    "    total_incorrect_sim = 0.0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_feat, text_feat = model.encode_tokenized_input(images, input_ids, attention_mask)  # (B, 512)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(\n",
    "                image_feat.unsqueeze(1),  # (B, 1, D)\n",
    "                text_feat.unsqueeze(0),  # (1, B, D)\n",
    "                dim=-1\n",
    "            )  # (B, B)\n",
    "\n",
    "            B = sim_matrix.size(0)\n",
    "            correct_sims = sim_matrix.diag()\n",
    "            total_correct_sim += correct_sims.sum().item()\n",
    "            correct_count += B\n",
    "\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "            incorrect_sims = sim_matrix[mask]\n",
    "            total_incorrect_sim += incorrect_sims.sum().item()\n",
    "            incorrect_count += incorrect_sims.numel()\n",
    "\n",
    "    mean_correct = total_correct_sim / correct_count\n",
    "    mean_incorrect = total_incorrect_sim / incorrect_count\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\" - Mean Correct Sim    : {mean_correct:.4f}\")\n",
    "    print(f\" - Mean Incorrect Sim  : {mean_incorrect:.4f}\")\n",
    "    return mean_correct, mean_incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 157/157 [11:22<00:00,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      " - Mean Correct Sim    : 0.6286\n",
      " - Mean Incorrect Sim  : 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_correct, mean_incorrect = evaluate_mean_similarity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후기단계 시작\n"
     ]
    }
   ],
   "source": [
    "print(\"후기단계 시작\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_attention(prev_attn, curr_attn, eps=1e-6):\n",
    "    \"\"\"\n",
    "    prev_attn, curr_attn: [B, T, P] - softmax된 attention map\n",
    "    \"\"\"\n",
    "    prev = torch.clamp(prev_attn, min=eps)\n",
    "    curr = torch.clamp(curr_attn, min=eps)\n",
    "    kl = (prev * (prev.log() - curr.log())).sum(dim=-1).mean()  # 평균 over T, B\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.ema = None\n",
    "\n",
    "    def update(self, value):\n",
    "        with torch.no_grad():\n",
    "            if self.ema is None:\n",
    "                self.ema = value.detach()\n",
    "            else:\n",
    "                self.ema = self.alpha * value.detach() + (1 - self.alpha) * self.ema\n",
    "        return self.ema\n",
    "\n",
    "    def reset(self):\n",
    "        self.ema = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloader, optimizer, device,\n",
    "#                 lambda_cons=0.05, lambda_self=0.01, warmup_epochs=1, num_epochs=3):\n",
    "#     model.train()\n",
    "#     prev_attn_map = None  # 직전 epoch의 마지막 attention map 저장\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss, total_acc = 0, 0\n",
    "#         progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#         # ✅ 에폭에 따라 lambda 값 조정\n",
    "#         lambda_bbox = 0.0 if epoch > warmup_epochs else lambda_cons\n",
    "#         lambda_self_eff = lambda_self if epoch >= warmup_epochs else 0.0\n",
    "\n",
    "#         for images, captions, masks in progress:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "#             # 1. forward\n",
    "#             img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "#             # 2. CLIP loss\n",
    "#             loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "#             # 3. BBox-based attention supervision loss\n",
    "#             T_mask = masks.shape[1]\n",
    "#             loss_bbox_cons = compute_consistency_loss(attn_weights[:, :T_mask, :], masks)\n",
    "\n",
    "#             # 4. Self-consistency loss (m-KL, 이전 epoch 기준)\n",
    "#             if prev_attn_map is not None and lambda_self_eff > 0:\n",
    "#                 loss_self_cons = kl_divergence_attention(prev_attn_map, attn_weights.detach())\n",
    "#             else:\n",
    "#                 loss_self_cons = torch.tensor(0.0, device=device)\n",
    "\n",
    "#             # 5. Total loss\n",
    "#             loss = loss_contrastive + lambda_bbox * loss_bbox_cons + lambda_self_eff * loss_self_cons\n",
    "\n",
    "#             # 6. Backward\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # 7. Metrics\n",
    "#             with torch.no_grad():\n",
    "#                 sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "#                 sim_mean = torch.diag(sim_matrix).mean().item()\n",
    "#                 sim_std = torch.diag(sim_matrix).std().item()\n",
    "#                 pred = sim_matrix.argmax(dim=1)\n",
    "#                 labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "#                 acc = (pred == labels).float().mean().item()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             total_acc += acc\n",
    "\n",
    "#             progress.set_postfix({\n",
    "#                 \"loss\": loss.item(),\n",
    "#                 \"clip\": loss_contrastive.item(),\n",
    "#                 \"bbox\": loss_bbox_cons.item(),\n",
    "#                 \"kl\": loss_self_cons.item(),\n",
    "#                 \"acc\": f\"{acc:.3f}\"\n",
    "#             })\n",
    "\n",
    "#         # ✅ 에폭 종료 시 기준 attention 갱신\n",
    "#         prev_attn_map = attn_weights.detach().clone()\n",
    "\n",
    "#         # Epoch summary\n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         avg_acc = total_acc / len(dataloader)\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "def train_model(model, dataloader, optimizer, device,\n",
    "                lambda_self=0.1,\n",
    "                warmup_epochs=3, num_epochs=5,\n",
    "                ema_tracker=None, start_epoch=0):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # EMA 준비\n",
    "    if ema_tracker is None:\n",
    "        ema_tracker = EMA(alpha=0.2)\n",
    "    use_ema = False\n",
    "\n",
    "    # 람다 스케줄러는 warm-up 동안만 사용\n",
    "    lambda_scheduler = LambdaScheduler(alpha=0.2, threshold=0.01, anneal_rate=0.01, max_lambda=0.5)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\", leave=False)\n",
    "\n",
    "        if epoch == warmup_epochs:\n",
    "            ema_tracker.reset()\n",
    "            use_ema = True\n",
    "            print(f\"[Epoch {epoch}] EMA 기준 self-consistency 시작\")\n",
    "\n",
    "        for images, captions, masks in progress:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            img_proj, txt_proj, attn_weights, _ = model(images, captions)\n",
    "\n",
    "            T_mask = masks.shape[1]\n",
    "            attn_weights_matched = attn_weights[:, :T_mask, :]\n",
    "\n",
    "            # ▶ Contrastive Loss (공통)\n",
    "            loss_contrastive = clip_contrastive_loss(img_proj, txt_proj)\n",
    "\n",
    "            # ▶ Warm-up Phase\n",
    "            if epoch < warmup_epochs:\n",
    "                lambda_cons, diff = lambda_scheduler.update(attn_weights_matched, return_diff=True)\n",
    "                loss_consistency = compute_consistency_loss(attn_weights_matched, masks)\n",
    "                loss = loss_contrastive + lambda_cons * loss_consistency\n",
    "\n",
    "            # ▶ Self-consistency Phase\n",
    "            else:\n",
    "                ema_attn = ema_tracker.update(attn_weights_matched)\n",
    "                loss_self_cons = kl_divergence_attention(ema_attn, attn_weights_matched.detach())\n",
    "                loss = loss_contrastive + lambda_self * loss_self_cons\n",
    "\n",
    "            # ▶ Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ▶ Accuracy\n",
    "            with torch.no_grad():\n",
    "                sim_matrix = F.cosine_similarity(img_proj.unsqueeze(1), txt_proj.unsqueeze(0), dim=-1)\n",
    "                pred = sim_matrix.argmax(dim=1)\n",
    "                labels = torch.arange(sim_matrix.size(0)).to(device)\n",
    "                acc = (pred == labels).float().mean().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "            progress.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": f\"{acc:.3f}\",\n",
    "                \"λ\": lambda_cons if epoch < warmup_epochs else lambda_self\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}, Avg Acc: {total_acc / len(dataloader):.4f}\")\n",
    "\n",
    "    return ema_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.9405, Avg Accuracy: 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.7593, Avg Accuracy: 0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.7222, Avg Accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.05,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=3,\n",
    "    num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.1864, Avg Accuracy: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.1822, Avg Accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.1588, Avg Accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.1380, Avg Accuracy: 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.1259, Avg Accuracy: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.1208, Avg Accuracy: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.1105, Avg Accuracy: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint_epoch3.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "prev_attn_map = checkpoint['attn_map']\n",
    "\n",
    "train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_cons=0.0,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=0,\n",
    "    num_epochs=7,\n",
    "    initial_attn_map=prev_attn_map,\n",
    "    start_epoch=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10:   0%|                                           | 1/1250 [00:00<16:05,  1.29it/s, loss=1.58, acc=0.000, λ=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LambdaScheduler] Consistency loss ON (diff=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 6.4236, Avg Acc: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 6.3687, Avg Acc: 0.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 6.3369, Avg Acc: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 4/10:   0%|                                                                              | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] EMA 기준 self-consistency 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.1345, Avg Acc: 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  73%|███████████████████████████          | 916/1250 [07:10<02:04,  2.68it/s, loss=0.0255, acc=1.000, λ=0.1]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.1156, Avg Acc: 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.0995, Avg Acc: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.1008, Avg Acc: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.0934, Avg Acc: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  69%|█████████████████████████▋           | 868/1250 [09:17<04:12,  1.51it/s, loss=0.996, acc=0.750, λ=0.1]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.0864, Avg Acc: 0.9720\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 전체 8 epoch 중 앞 3 epoch은 warm-up, 이후는 self-consistency\n",
    "ema_tracker = train_model(\n",
    "    model, dataloader, optimizer, device,\n",
    "    lambda_self=0.1,\n",
    "    warmup_epochs=3,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cu118env)",
   "language": "python",
   "name": "cu118env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
